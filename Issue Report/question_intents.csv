rootcause,subcategory,symptoms
Unclear Installation Guidance,System Support and Dependency Management,"Documentation missing or unclear issues,Environment dependency issues (CUDA/triton etc.),Log/warning message explanation needed,CI/build process issues,User raises questions about installation or environment related errors,Error messages unclear or fix path unknown,Undocumented default behaviors,Subprocess error handling questions,Dependency missing error handling,Documentation completeness concerns,Installation process anomalies,Documentation inconsistent with code,cpu-only host installation/docker security inquiry,Inference failure environment configuration check needed,vllm docs unclear leading tool failure,AMD CPU installation/GPU type inference issue,ImportError undefined symbol resolution,Environment version mismatch leads to exception,transformers chat template missing handling,Documentation update request,Dataset preparation guide supplement needed,Multi-node distributed initialization failure,User needs clear C++ version requirement for toolchain,Lack of startup command examples,Missing example code,API documentation missing,User questions doc/toolchain usage flow,Involves unittest zero configuration issue,User installation configuration failure inquiry,Request deployment tutorial documentation,Dependency version mismatch error,Docker configuration failure issue,Lack of benchmark build guidance,Third party library version question,User requests additional tutorial docs,User requests project acknowledgement info addition,User asks release date,User cannot find feature description doc,User finds missing dependency version,Dynamic network explanation missing,Incomplete documentation leads to operation failure"
Unclear Installation Guidance,Configuration and Parameter Usage Issues,"Parameter description missing (such as combined call method),Parameter setting method missing (num_crops/stream_options),BF16 build parameter setting question,Performance comparison configuration disclosure requested,Interface usage unclear causes confusion"
Usage Gaps in LLM Operations,User Support and Documentation Quality,"Users do not understand the purpose and enabling method of a specific feature,Users unclear about parameter meaning or reasons for inconsistent output,Users do not understand the purpose of a specific feature or module such as injection_policy_tuple,Users ask how to use a particular feature or model such as FusedAdam or OPT loading,Alternative solution inquiry,Lack of understanding of constraints leads to errors,Question about operation detail such as whether leaky_relu error is misuse,Question about operation flow such as steps to run BingBertSquad,Variable purpose inquiry such as what _doc_count does,Inquiry on how to set long-text input for Llama3.1 405B,gguf model extension loading requirements,Inquiry about deprecation plan for old ZeRO version,Consultation on hardware choice for PPO,How to load and test DeepSeek 671B,Inquiry on how to enable features such as activation checkpoint offload,Minimum memory requirement consultation for pretraining or fine-tuning,User asks detailed steps and dataset usage for knowledge replay stage,User asks training steps or underlying technical principles,User asks how to configure features such as gradient accumulation or parallel strategy,User has questions about configuration interface or operation steps,User questions training process steps,User confused about PR operation steps,User asks how to save or load a model,User asks parameter purpose and performance impact,Question about model format conversion method,User asks preprocessing steps or dataset replacement,User asks about framework design principles,User confused about setting sampling parameters,User asks about internal implementation details of the framework,Question about how to compute performance metrics,Confusion over quantization type selection,User asks how to use an existing feature,User asks how the feature is implemented,User asks about the underlying mechanism principles,User asks parameter optimisation methods,User asks how to release GPU memory,User asks how to set performance parameters such as generated tokens or throughput,User not familiar with static memory test method,User asks about fused operator development roadmap,User does not understand distributed training steps,User confused about technical terminology,User consults model fine-tuning steps,Ambiguity in fine-tune data format for users"
Usage Gaps in LLM Operations,Functionality Support and Boundary Clarification,"Users ask how to enable a feature,Users uncertain whether a configuration affects training metrics,Technical issues about parameter settings,Interface parameter usage question,API call parameter validity question,Users question configuration effectiveness or why it had no effect,Question about parameter combination feasibility such as MOE+SD compatibility,Model load failed and cannot locate reason such as structure or path mismatch,Question about concurrency control of training processes,Issue enabling inference acceleration techniques such as quantization or pruning,Question about difference in parameter configuration such as definition of max_micro_batch_size,API input format question such as ds.initialize accepting dict,Feature availability question such as whether eval_batch is usable,Question about parameter control mechanism such as min_tokens generating insufficient length,Question about reasonableness of parameter setting such as max_seq_length exceeding limit,Question about impact of inference parameter changes such as added parameter affecting speed,Question about randomness control such as whether temperature=0 fixes result,Users ask whether caching logits is required and why structured output becomes slow,Consultation on DistStoreError solution or local model usage permission,Consultation on model conversion quantization configuration or format support,Question about validity of parameter settings such as max_pixels or model_tag,Consultation on feasibility of using Gemma2 as a classification model,Consultation on embedding model configuration or controlling XPU misdetection,Sampling parameter results not as expected,Question about auto tool selection parameter truncation mechanism,Solution needed for repeated GGUF output,Inquiry about scenario where ScaledActivation not implemented,Question about performance metric calculation such as tokens per second,Consultation on GeminiDDP strategy composition,Inquiry about method to implement dual loss function,Inquiry about reusing LoRA during training stage,Inquiry about status of SFT fine-tune code,Consultation on applicability of GRPO with LoRA,User integrates custom training flow question,User unclear on multi-turn conversation training method,User asks about EMA implementation in model sharding scenario,User verifies enable_kv_cache_reuse method,Comparative question about quantization implementation details,User questions component call logic,Consultation on DSR1 performance practice,Question about throughput metric comparison,User compares technology suitability scenarios,User questions parallel strategy or inference flow configuration,User consults long-context inference optimisation methods,User has questions on parameters or code logic,User compares framework performance metrics,User adjusts limit on number of checkpoint saves,User needs customised log output"
Unexpected Behavior and Weird Design,Framework Implementation and Design Issues,"Parameter/API behavior inconsistent with expectation,Mixed precision related issues,Distributed strategy behavior questions (Zero/DDP/TP etc.),Abnormal initialization or quantization behavior,Configuration combination anomalies (TP/Optimizer/Checkpoint etc.),Training process anomalies (gradient vanishing/weight distribution abnormal),Model structure restoration issue,API call recursion or error without explanation,Offload strategy switch ineffective,Performance drop for unknown reason (e.g. 1bitlamb/ZERO3),Parameters ignored (e.g. launcher_args/offload),Initialization failure (init_inference crash/out of memory),Distributed strategy detail question (e.g. whether ZeRO supports SGD),Question on training step order impact (e.g. optimizer/lr order),Question on underlying implementation principles (e.g. PostBackwardFunction uses detach),Module behavior anomaly (e.g. linear overwritten),Question on initialization logic reasonableness (e.g. only rank0 initializes weights),Abnormal training effect (e.g. max_grad_norm ineffective in FP16),Assertion failure triggered by parameter change,Model structure compatibility question (e.g. missing key when dist-ckpt init),Module output consistency question (e.g. GLU activation TP mode difference),Algorithm implementation difference question (e.g. Megatron vs HF TopK),Early training anomaly (e.g. GroupedMLP initial loss too high),Random state synchronization question (e.g. TP not sync random),Question on API feature ownership (e.g. ring_exchange custom or not),Question on correctness of underlying implementation (e.g. _CopyToModelParallelRegion),Output exception handling question (e.g. stop parameter causes content anomaly),Cache mechanism question (e.g. disable_mm_preprocessor_cache),Default behavior question (e.g. whether CPU usage is limited),max-model-len impact on short text inference question,Modified model registration failure issue,Old model service BlockAllocator error analysis,Llama3.2 tool OpenAPI call failure,Question on mechanism of numeric error for mismatched input shape,Attention module or LoRA inference speed drop analysis,QLoRA adapter inference correctness question,Model load speed anomaly (2B versus 7B comparison),Analysis of reason for multiple free_seq calls,Fine-tuned model load anomaly,Stream tool inconsistent response question,Performance degradation after version upgrade analysis,quantization=fp8 load fail or GPTQ bias impact,StarCoder/LoRA inference speed anomaly,Inference difference analysis after LoRA enabled,Multimodal tensor stacking anomaly question,echo=True output format anomaly question,Critic class design challenge,Feedback on PPO parameter convergence difficulty,ChatGPT module naming hard to find,max_length parameter meaning unclear,Custom token adapter tokenizer issue,Question on missing terminology definitions"
Unexpected Behavior and Weird Design,Resource Efficiency and Optimization Management,"Training or inference performance issues (not converging/slow/low accuracy),GPU memory or RAM anomaly,Memory management issue (OOM/offload abnormal),Resource usage or speed below expectation (training or inference efficiency low),Hardware resource utilisation anomaly (e.g. some devices unused),GPU utilisation anomaly (e.g. only single card executes generate),Performance bottleneck question (e.g. transformer kernel backward slow),Scaling efficiency below expectation (e.g. MoE model DP scaling ineffective),Memory behaviour anomaly (e.g. FP8 training OOM while bf16 fine),Memory usage anomaly (FP16 larger than FP32),Inference speed drop question (e.g. slower after version),bitsandbytes quantization performance drop question,Need to locate inference speed bottleneck,OOM despite sufficient memory allocation logic question,cpu-offload memory not reduced anomaly,Hardware utilisation anomaly (e.g. CPU usage cannot exceed 100 percent),beam_search slower or Mixtral accuracy question,batch_size related anomalous behaviour,Offload mechanism dynamic impact on training question,Mixed parallel memory usage comparison question"
Unexpected Behavior and Weird Design,Model Behavior and Output Verification,"Results differ from expectation but user cannot confirm if bug,Correctness of output questioned（e.g. different from HuggingFace）,Training dynamics abnormal（e.g. high learning rate causes loss increase）,Accuracy mismatch after training resume,Loss curve differs（caused by different ZeRO stage）,evaluate interface gives inaccurate evaluation result,Correctness of reward_model truncation questioned,FP16 softmax compatibility / loss consistency question,Non reproducible generation result question,tokenizer output consistency / missing delta field question,Stability of spec decoding result questioned,Output content abnormal（first chunk empty / memory balance）,Batch size influences inference result question,Output exception handling question（e.g. stop parameter causes content anomaly）,Inquiry about optimizing vllm inference speed,max_length parameter meaning unclear,User questions whether weight quantization precision takes effect,User questions training efficiency or effect,User cannot obtain model or tokenizer"
Cross-Platform Deployment Issues,Engineering Implementation and Deployment Challenges,"CUDA error when running multi node,Feature support inquiry（e.g. MoE convert to FP32）,Feature compatibility question（e.g. ZeRO3 with Albert）,Parameter compatibility crash（e.g. Zero stage parameter conflict）,Feature compatibility question（e.g. Zero2 with pipeline）,Model compatibility question（e.g. reason for DeciLMConfig error）,Feature support inquiry（e.g. whether Greedy sampling supported）,Process isolation issue（e.g. LLM.collective_rpc failure）,System behavior impact question（e.g. fork affecting Ray actor）,Kaggle local model usage failure / VLLM training not converging,Deployment anomaly hardware compatibility question for Deepseek-R1,Qwen2.5-VL support question（architecture limit / multi GPU）,Ray cluster service start failure / inference slow analysis,GPU utilisation drop with concurrent requests question,Feature support inquiry（disable flash_atten / TP performance impact）,vllm model compatibility / type error resolution,Model add failure / confirm Molmo support,FP8 kv cache and flash attention compatibility question,Path to solve Neuron unsupported model structure,Multi GPU or TP mode inference failure config question,NCCL parallel error handling consultation,vllm run failure in K8s environment analysis,Qwen2.5 long text or quantization support consultation,IPv6 or flashinfer long text support question,Triton error / streaming count anomaly analysis,tool_choice failure / rollback version effective analysis,ProSparseMiniCPM1B component dependency question,BitsAndBytes support or offline chat method question,Hardware spec impact on inference speed analysis,torch version multimodal support consultation,User cannot load Meta or HuggingFace weights,User uncertain about Ray PPO maintenance status,User uncertain about multi model deployment support,User confirmation of framework feature support,CPU build feature lack,User confirmation of hardware or software support,User inquiry about model or quantization method support,Multi GPU inference configuration consultation,CUDA version compatibility confirmation,User confirmation of quantization type support,Model support status inquiry,User confirmation of feature development plan,User confirmation of environment compatibility,User inquiry about version feature differences,User confirmation of hardware or model or LoRA conversion support,User confirmation of feature availability,User inquiry about model adaptability（hardware or framework version）,User inquiry about CPU inference feature support,Distributed scenario run error,User confirmation of model or feature support,User confirmation of hardware deployment feasibility"
Cross-Platform Deployment Issues,Environmental Compatibility Challenges,"Feature support inquiry (hardware/version/scenario),Cross-platform compatibility issue (OS/hardware/version differences),Specific acceleration tech compatibility issue (NPU/AMD etc.),Framework conflict issue (ColossalAI/accelerate etc.),Version upgrade discrepancy issue,Cross-platform installation or behavior difference (Windows/Linux/3090 hardware support),Hardware compatibility inquiry (e.g. 3090 loading T5-11B),Version dependency conflict (e.g. new transformers ImportError),macOS/ROCm platform support consultation,ROCm platform support confirmation,CPU support/f16 instruction compatibility consultation,Specific scenario feature support consultation (FP8-MoE etc.),Offline environment run feasibility consultation,Cross-device porting feasibility question,User question on framework and external tool compatibility (Docker/PyTorch version),User question on MegatronLM adaptability,User large-model migration failure (CANN doc operations),User asks about NPU feature support/framework switching method"
