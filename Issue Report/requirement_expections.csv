rootcause,subcategory,symptoms
New Environment Compatibility and Deployment,Hardware Adaptation and Compatibility Requirements,"For example requests to support new hardware such as XPU、AVX2、HPU,Requests to add new features such as Thunderbolt、multi-node training、AVX2、XPU、FP8,Desire to support environments like NPU、ROCm、Windows,Desire to support accelerators or platforms like XPU、NPU、Ascend,Incomplete support for XPU/NPU/AMD ROCm and hard-coded IDEX issue,Desire to support new hardware such as NPU、AMD、Windows,Desire to support AMD GPU、HPU、torch.compile and similar,Request to run inference/training on new platforms,Such as Windows installation、fp6 quantization support、automatic partitioning,Suggestion to adapt Windows、WSL、docker environments,Suggestion to support more CUDA/PyTorch combinations,Request to support Blackwell GPU、Apple Silicon、Windows platform,Request to support new hardware architectures like Power CPU、Ascend、WSL2,User requests improved compatibility for Windows、MPI、Docker platforms,User wants to compile/run on local cluster、Windows or Jetson,Current build_win.bat compilation fails,Current hardware cannot run or outputs abnormal,Require framework to work on non-NVIDIA GPU or cloud environments,Request compatibility for M1/M2、ROCm、MI200 hardware or alternatives,Require support for non-standard hardware like AMD CPU、MI200、Windows、M1/M2 Mac,Desire support for build needs on Windows、ROCm、AMD GPU、no-GPU environments,Request official support and validation for inference on Intel or AMD platforms,Cannot run on specific device (XPU),Support Windows installation / fix ROCm build / CI adaptation,User cannot run or deploy model on specific hardware or platform,User requests support for specific hardware such as CPU-only inference、RISCV architecture or GPU setups like multi-GPU parallel,Cannot accelerate on devices like Movidius Compute Stick or Pine64 Quartz64,User asks feasibility of training/inference on specific hardware such as Atlas800A2/910A,Lack of official x86 image prevents inference on x86 machines,Cannot achieve multi-GPU parallel inference or training over gigabit/10-gigabit networks,TensorRT engine build fails due to insufficient memory、user asks required resources,User requests support for specific platforms such as Windows libraries、cuda12.5、MPI alternatives,User requests hardware/framework compatibility like Windows script support、T4 GPU memory optimization、multi-GPU architecture support,Need to add NPU device support and multi-architecture GPU build support,Need a unified interface or wrapper for multiple hardware platforms such as NPU、GPU,Request support for older Python versions、Windows build optimization、compatibility with more GPU architectures,Need to run latest software environment on H800 GPU,User requests hardware/framework compatibility support such as Windows script support、T4 GPU memory optimization、multi-GPU architecture support"
New Environment Compatibility and Deployment,Deployment and Toolchain Optimization,"For example suggest independent XPU build path、expose missing dependencies earlier in build,User wants improved initialization or shared memory support in swarm/docker,Proposal to upgrade CI support environment,Docker image lacks latest tag、ColossalChat model weights not released,User suggests adding git-lfs dependency to handle binary files,Docker commands and image names need update,User requests adding Docker support for SSH/RDMA,User wants to skip engine build step and run model directly,Need to upgrade components such as Triton kernels version or refactor code to fit new and old versions,Need to update Docker build process、add tensornvme dependency and other environment configs,Need to compile install package for Windows system,User wants support for CUDA 12 and Torch 2.2 but current code uses old versions,Install module versions via pip extras or flag control for Triton、sparse-attn etc.,User expects pip install and run to work on these platforms,Avoid failure in virtual machine or test environments,Improve cross-platform build and installation experience,Environment setup process needs unified standard,Cannot install via pip or packaging tests fail"
New Environment Compatibility and Deployment,Dependency Version and Compatibility Management,"Need to upgrade dependency versions,Need to upgrade transformers version and update related model implementations,Must upgrade Transformers to 4.39.1、synchronize code branches、resolve version dependency conflicts,For example setuptools >=70 causes install failure、recommend compatibility with numpy >=2.0,Currently only "cuda" policy supported、need to extend "cpu" and "auto",Need to support new PyTorch versions、add torchrec compatibility tests、support PL integration and other framework adaptations,Adapt to newer versions of Pydantic 2、Torch nightly、Transformers,Adapt to new Transformers、Peft、SFTTrainer、HF hub code structure changes,Adapt to OneAPI 2025.0 toolkit,User requests upgrading onnx from 1.12.0 to 1.13.0,Project dependencies unsynchronized or version conflicts,User proposes upgrading TensorRT-LLM version or dependency versions such as CUDA or TensorRT,User requests support for specific hardware such as RTX4090 sparsity optimisation or missing GPU driver,User reports Python dependency versions pinned too strictly making integration difficult,Need multi-version compatibility such as Python3.6 adaptation and CUDA or Torch version build scripts,Need to add module version validation feature,Official maximum version mismatched with dependency versions causing compatibility issues,requirements.txt missing necessary packages,cuDNN version not specified causes runtime error,Need to remove MPI dependency or update libcutlass library,Library version conflict such as GLIBC_2.38 requirement prevents old systems from running,Build process not covering specific needs such as Jetson platform compatibility or PyTorch version lag in Docker,User encounters dependency version conflict such as transformers version between vLLM and TensorRTLLM"
Functionality Support for LLM Development,API and Interface Requirements,"Correct type hints,Support export interface,Add new API,initialize should not alter model semantic behavior,Use new norm API consistently,state_dict portability,Replace lmhead,Log format,Allow skipping unsupported modules,Support num_key_value_heads configuration,More general tensor type detection,Direct support for HF training scripts,Support checkpoint,Tokenizer component support,Function state not handled properly,Count only regular parameters,Allow disabling default behavior,Finer grained pipeline configuration,Support longer sequences,Auto detection of multiple checkpoint types,Dynamically add tokens to model,Simplify API usage,Unified behavior,Align with HF to avoid discrepancies,Nested checkpoint support,Avoid shard loss,init error messages,Control dropout in eval mode,Config accepts dict parameters,More robust checkpoint rebuild,Improve config print format,Support skip module parameter handling,layer-norm-eps parameter,ds_elastic execution permission,User friendly interface for activation checkpoint,PartitionedTensor replaces attention mask gradient,Clearer layernorm epsilon behavior,inject training state cleanup,Restore test matrix,Reasonable pooling method,KVCache params taken from model config,Tokenizer fallback behavior setting,Unify transformers and vllm output differences,Integrate Pooler config parameters,Qwen2VL supports id_to_byte_piece,Support token proposals of different lengths,ng-gram and dynamic decoding,Support Hugging Face continue_final_message parameter,Switch to disable prompt text output,Load model file without extension,Training gradient norm info missing,RPC server support,Ignore EOS token logic,bias_gelu_jit_fused basic feature,alibi_slopes parameter passing,flash_attn_varlen_func adaptation,get_cos_and_sin kernel math computation,subgroup parameter support,Custom mask functionality,gradient_checkpointing_ratio,Extend colo attention API interface,Compilation does not support .separate operator,HuggingFace Accelerate feature integration,Streaming Output API,Sync or async API server,FastAPI integration,vLLM test support,Save or load sharded models,Padding free modeling,Speculative Decoding implementation,Add Drafter model container,DistCrossEntropy module support,RMSNorm support,Any batch size support,Add inference engine,Demo case design,KV-Cache management,Replace Adam optimizer with SGD,Lora feature API missing,Dynamic batch inference,Support segmenting truncated long texts,Improve parameter interfaces,Add extra parameter support,Convert checkpoint to HuggingFace format,Integrate wandb support,New dynamic batching feature,Support specific features like gradient accumulation、mixed parallel、custom strategy,Merge flash attention branch into pipeline,Merge shardformer model into pipeline,T5 model pipeline test support,Special pipeline allocation algorithm for T5 structure,chat module tensorboard support,Integrate GPT model pipeline strategies,ColoAttention attn mask type support,paddedcausal mask type,Save or load shard state_dict compatible with HF,Save or load checkpoints for non-sharded optimizers,no_sync or DTensor or optimizer checkpoint features unavailable,Gemini or ColossalAI plugin does not support shard checkpoint,train_prompts script max_len multiple tokenizer and verbose parameter missing,inplace add operation missing,Lack of sharded checkpoint support prevents loading HF sharded models,vf_coef parameter missing makes PPOTrainer unable to compute value loss,Required features missing cannot satisfy user needs,Repetition penalty setting missing lowers generation quality,autoparallel related API call failed,Generation API missing num_return_sequences parameter,User expected operation needs cannot be met,Hook integration example report generation and performance analysis feature missing,Target model or module feature unusable,Expected feature unusable or demo missing"
Functionality Support for LLM Development,Performance and Resource Optimization,"VRAM usage prediction,Automatic tuning,Improve training speed,Improve memory efficiency,Replace pynvml with torch built-in API,Reduce dependencies,Improve flops_profiler,Configurability of profiling tools,Acceleration effect below expectation,Dynamically switch fused/cpu optimizer based on device,Avoid loss overflow,Monitoring metrics or granularity,Intelligent memory management during checkpoint loading,Optimize see_memory_usage tool,Suggestions for cpu_offload settings,see_memory_usage improvement,Stability optimization for cpu_adam component,Reduce model test size,Transformer engine layer performance,Refactor layout to boost inference speed,Auto budget estimation for grid search,Optimize load-pin overlap,Non-blocking pin loading,Sequence parallel performance optimization,Memory usage optimization for multi-GPU training,Refactor cross-entropy computation,Plugin support needed for training OOM issues,Data prefetch/async operations,CI time performance improvement,Optimize reduce scatter operation,CUDA performance optimization for KCache layout,Inference module performance optimization,CPU offload optimization on Zero1/2 platform,Large-scale optimization,Optimizer offload acceleration,Padding vocabulary size algorithm optimization,CUDA kernel performance optimization for rmsnorm algorithm,CUDA Graph support,Model generation process optimization,Rotary Embedding acceleration,Decoder kernel performance optimization,Linear operator speed optimization,Speculative decoding acceleration,Real-time TFLOPS display during training,New load balance feature,Performance or features such as Flash Attention 2、overlap of input collection and gradient computation,Parameter storage such as moe parameter tuning、placement policy optimization,Forward inference computation optimization,In-place sharding optimization,Training time optimization such as zero optimizer steps,Shardformer performance evaluation support,Small model non-parallel optimization support,Performance drop due to unsupported normalization kernel,Lazy initialization causes large checkpoint processing failure,Low memory/8-bit training/zero2 CPU VRAM-friendly mode,Benchmark does not support large models or specific models"
Functionality Support for LLM Development,Data Support and Processing Requirements,"Long input text support,lazy dataset mode support,Cached data format,Distributed data loading support,Safety evaluation dataset,Missing database conversation access feature,Missing unsupervised learning/Chat application evaluation/Community Pipelines"
Functionality Support for LLM Development,Model Expansion and Compatibility,"Support specific model interfaces,Support InfV2,Support MoE,Add error message when using Triton with BERT models,Llama2 grouped attention,MoE and PR-MoE example run failures,Support OPT model,Support BloomZ model,BetterTransformer support,Support new model architectures,Improve Hybrid Engine support for LLaMA,Integrate InferenceEngine with models,Support specific models such as Flan-T5,Student-Teacher architecture,OPT-66B support,MoE configuration,BLOOM inference,checkpoint loading adapted to HF structure,Support loading models after structural changes,Support XLMRobertaForSequenceClassification,Multimodal model outputs 2D tensor,Mixtral embedding support,Qwen2VL supports more visual formats,Running llama 3.2 on CPU requires cross attention and encoder-decoder,Multi-image input error needs multimodal expansion support,Adapt new models such as Llama405B,Integrate new algorithms into MoE models,RL-style generation,Gemma2 or llama model NotImplementedError,Integrate new component such as Liger-Kernel to boost training,Shadow expert feature,MOE hybrid plugin,Framework supports specific model such as qwen,Support CommandR model,T5ForTokenClassification support,qwen model support,MoE module refactor,Qwen2 model adaptation,Baichuan 7B/13B TP support,LLaMA-3 CPT/ST support,MoE module reconstruction,Replace MoEManager with MoeHybridParallelPlugin,Remove dependencies,Shardformer module update,GaLore technology integration,NPU support,Multi-architecture build support,MOE module support,NPU communication acceleration,Withdraw Mistral support,Not adapted for Falcon/Mistral/Llama models,New model support such as GPT2 sequence parallel、TP+ZeRO、ring attention,Hierarchical Gemini support,GLM/ChatGLM model support,Support LLama、Blip2、SAM、T5、GPT2、OPT、BERT、Bloom、Whisper models,Shardformer module features such as inplace sharding、lazy init、flash attention support,VITT feature missing,Bloom/BERT/GPT-2/T5/LLaMA models not supported,ChatGLM-6B/BLOOM/seq2seq model training inference support missing,vit model support missing"
Functionality Support for LLM Development,Training Workflow and Configuration Optimization,"More unified optimizer behavior,Unified checkpoint format,Freeze partial weights,Submodule finetune configuration,Dynamic freezing,Training support features,Gradient merging,Automatic loss scale adjustment,ZeRO2 checkpoint cannot restore weights,Provide better default configs,Automatically set environment variables,Larger batch,More flexible initialization,Warmup schedule supports slow curve,Optimizer state loading,Improve pretraining and inference scripts,Support multiple backward passes,Implement optimizer checkpoints,Missing num_microbatches support,Support DPO training,Support MCTS algorithm,Cannot choose checkpoint save frequency during training,Optimizer integration,Multiple backward pass support,Training script optimization,Evaluation module optimization,Auto resume after training interruption,Gradient accumulation feature extension,Weight saving feature extension,Conversational training support missing,Sharded optimizer checkpointing missing,Distributed PPO trainer missing,Resume training feature missing,Gradient accumulation missing GeminiDDP cannot accumulate,Checkpoint save callback missing"
Functionality Support for LLM Development,Distributed Support Requirements,"hostfile alias support,Incompatible activated tensor communication logic,Integrate DeepSpeed with mainstream community tools,Set reduce_bucket_size,Communication overlap,Zero++ weight sharing,Checkpoint loading adapts to new nodes,Sharded checkpoint support,Worker elasticity,Inference on all GPUs,Flexible launcher parameters and process control,Enhanced single GPU checkpoint to CPU,Tensor parallel communication overlap,Reset tensormodelparallelsize,Multithread tensor parallel acceleration support,New pipeline parallelism features,New distributed checkpointio features,Async IO support,Distributed environment features such as GRPO training support,ProcessGroupMesh multi node support,Plugin async API such as async checkpoint,Sync IO mechanism performance bottleneck,Tensor Parallelism support missing,Zerobubble and similar training mode support missing,MoE hybrid parallel,Hybrid parallel strategy support,Hybrid parallel plugin integrated with Shardformer,all2all communication operator optimization,PP plugin support,Support 4D parallel,Distributed optimisation strategies,Zero offload feature improvement,Automatic distributed optimizer conversion,Shardformer parallel output support,Distributed optimizer integration,Support DDP distributed training,Support ZeRO distributed training,Distributed implementation,Tensor Model Parallel inference support,Heterogeneous sharding strategy,dit model layer parallel,TP logic support,Hybrid parallel model expansion,Parallel strategies such as TP+DP hybrid/sequence/pipe parallel,Multi model collaborative training,Hybrid parallelism support,Sequence parallel operation support missing,3D parallel plugin support,1f1b scheduling phase management and P2P communication support,Parallel features such as Tensor/Pipeline Parallelism unavailable"
Functionality Support for LLM Development,Quantization and Inference Support,"Support fp8,Support bf16,Support INT4,Mixed precision schemes,Avoid automatic precision casting,Isolated and controllable inference states,Optimise BF16 gradient accumulation,Improved inference stability,LoRA compatibility,Flexible ZeroQuant settings,Support GPT-J int8 mode,fp16 crash under high learning rate,Nonstandard input tensor fp16 config,AMP conflicts with ZeRO,Meaningful logits output,Enhanced zero_to_fp32,More robust fp16 control logic,bias add precision fix,BF16 optimizer cur_scale check,MiniCPMV supports bitsandbytes loading,fp8 slower than fp16 at batch=1 requires tuning,FP8 communication support,FP8 async communication,FP8 benchmark support,LoRA training parameter options,Shardformer llama model FP8 training support,FlashAttention compatibility,fp8 conversion support,Quantization cache interface,LoRA quantization support,Quantised kvcache for FlashDecoding,Support Triton new kernels,Lora Quantization Support,High precision flag for Rotary Embedding,Flash Attention patch,LoRa feature support,New smoothquant feature,FP16/O3 config unsupported,Flash Attention 2 support,Pure bf16 training unsupported,New Flash Attention v2.0.1 support,FP8 mixed precision training,Transformer Engine FP8 mixed precision support,Flash Attention enable failure,low level plugin BF16 mixed precision training unavailable,bfloat16 mixed precision shard checkpoint features unavailable,GeminiDDP cannot save checkpoint in FP16"
Code Reliability on supporting LLM of DL Frameworks,Code Quality and Architecture Optimization,"Need to improve graph compilation stability、avoid breakage and internal-structure incompatibility,Communication-computation overlap logic needs optimization、op compile optimization flags should be used cautiously,Avoid manual injection rules or Bloom errors、enhance AutoTP intelligence,Improve multi-node Pipeline or ZeRO communication stability,Hope auto-tuning tool generates more reasonable configs、avoid missing fields such as offload,Parameter setting inflexible (e.g. benchmark uses absolute path instead of parameter),Variable or class naming confusion causes comprehension difficulty,Module inefficiency or resource imbalance (e.g. load imbalance、insufficient inference performance),Need to optimize underlying logic (e.g. async control、sparse matrix operations、kernel optimization),Need to update old API imports or perform architectural adjustments,User proposes merging modules to optimize inference performance or reduce duplicate code,Code readability poor or does not follow development conventions,Existing implementation has performance bottlenecks (e.g. MetaInfoProp too time-consuming、ZeRO process communication imbalance),Existing code structure needs adjustment (e.g. CommSpec class should be decoupled、strategy generator should be templated),Auto parallel processing has redundant strategies or wrong physical shape mapping,Code not conforming to standards needs formatting adjustment,Need to optimize main loop to avoid busy-waiting,User proposes adding pipeline test cases for models like Bert/OPT,Need to remove unused Triton kernels、clean code structure、fix naming issues、refactor gradient hook management,User suggests optimizing ColoOptimizer initialization method code structure,User requests optimizing build script coding style guidelines,User requests improving ColoTensor and similar classes __repr__ implementation,Need to refactor Layer integration code and 2.5D layer code to improve accuracy,Need to optimize lr_scheduler naming logic、improve issue template usability,Optimize coding style for files like common.py,Reformat code via autoflake and isort,Existing parameters cannot directly control memory/performance (e.g. batch settings、quantization params、parallel mechanism)、need to optimize compute bottlenecks or resource management,Users report model compilation performance insufficient or memory usage too high、request config optimization,Users suggest optimizing communication/computation logic (e.g. replace linear ops with Shardformer、improve P2P communication),Users propose improvements such as KV cache transfer metrics、memory optimization、technical optimization directions,Users require splitting pass modules and adding unit tests,Code readability poor or style inconsistent needs optimization,Code style inconsistent needs optimization (e.g. files builder.py、cpu_adam.cpp),sample folder or directory structure does not meet development workflow needs refactor,Need to refactor old code or remove legacy code,Parameter sharing leads to inconsistent compute modes,Code duplication or redundancy needs design optimization,Hard to extend and reuse,Core code not decoupled from new features、leads to poor modularity and extensibility"
Code Reliability on supporting LLM of DL Frameworks,Test Automation and Quality Assurance,"Recommend adding runtime profiling、model validation、OOM analysis and other tool support,Low-memory path、libaio detection etc. not covered in unit tests or CI,All error messages should include module name、suggested action、severity level,Kernel modules such as bias_add、relu、quantization need accompanying unittests and test fp16/fp32 modes,Build scripts should check dependencies such as libaio、OrderedDict,Need to replace current performance regression logic / avoid flaky tests,Formatting tools need improvement、unit tests need supplement,New modules or features lack unit and integration test coverage,Need to simplify test cases / add benchmark tests,Test model library incomplete,Chaotic test folder structure causes CI test failure,CI environment skips new algorithm tests,Insufficient test coverage leads to unstable functionality after code changes,Need to add test validation (e.g. offline continuous batching tests、NPU code tests),Lack of test cases (e.g. model forward accuracy test、Mistral test),Need to add checkpoint tests、update test files to suit single-GPU environment,Missing interface test cases leads to insufficient quality assurance,Test coverage incomplete or example run fails,Users propose enhancing test coverage (e.g. add accuracy test scripts、migrate test cases、add conditional decomposition tests),Need to unify auto parallel code formatting"
Code Reliability on supporting LLM of DL Frameworks,Debugging and Logging Requirements,"Recommend adding log output、diagnostic tools or configuration validation,Recommend improving API interface or log readability,Hope logs are clearer、deprecation warnings migrated earlier、error messages more user friendly,Hope trace cache controllable、diagnostic info more explicit、decorator behavior traceable,Recommend adding more assertion checks、improving error messages、enhancing debugging usability,Hope error messages more precise such as missing f string、trace clearer,Request enhanced diagnosis and reproduction tools for deadlocks、grad=None、hangs,Recommend improving log readability、strengthening parameter checks、preventing NaN propagation,For example hope to add error prompts、optimize log output、support suppress noisy print,For instance add model initialization logs、optimizer memory usage prints、clearer error prompts,Recommend adding log_summary、avoiding duplicate outputs etc.,Hope to record finer grained logs for optimizer、offload、loss etc.,Hope to reduce log printing、accurately estimate iteration time,Hope to optimize log output format or add debug info,Request adding debugging functions such as checkpointio debug log recording,For example hope backend process errors return correct HTTP status codes、logs not sent to stderr,For example hope logs streamlined or structure clearer,Current code contains print and needs standardized logging behavior,Users hope to reduce redundant logs、print only on global rank0、support adjustable log levels on demand,For example users hope to disable debug printing、clean extra stdout info etc,Request renaming confusing log messages"
Code Reliability on supporting LLM of DL Frameworks,Exception Handling and Fault-Tolerance,"Propose enabling fallback logic when configuration conflicts or feature unsupported,Expect fallback mechanism、prevent uninitialized usage and similar issues,When autotune fails configuration can downgrade instead of exit,For example users suggest adding rollback mechanism when xGrammar speculative decoding crashes to avoid service interruption,For example users hope rollback when Speculative Decoding crashes to avoid service disruption,Initialization parameters should be auto derived or provide fallback to avoid failure due to omission,Need to avoid NoneType exceptions slash check boundary values,For instance hope clear prompts or fallback when quantization fails、CUDA errors or KV cache insufficient,Suggest more friendly error when placeholder truncated,For example hope to improve RPCServer error throwing and enhance clarity of exception prompts,For example Abort mechanism should work in Streaming and logprobs should be consistent in multi step inference,For example multi path model loading should tolerate path errors and retry,Need to add exception capture and retry feature,Need to add exception capture and retry feature"
Community and Document Support,Collaboration Workflows and Toolchain Support,"Hope CI output is clearer、add failure type hints、keep documentation notes in sync,Recommend CI installs lmeval and other dependencies from latest source directly,CI pipeline should dynamically pull latest transformers/lmeval branches to avoid breakage,Suggestions such as relaxing xformers version pin、letting flashattention support offline build、asking for clearer logging and field checks,Require updating PR workflow and doc standards (e.g. add self-check list and standard steps),Propose upgrading CI/CD toolchain (e.g. pre-commit version bump),PR review and contribution process lacks standardized checks,Dependency version bumps (pre-commit or black) cause format checks to fail,Need extra pre-commit config and PR compliance checks,Missing Docker CI stage and standard contribution flow,GitHub Actions flow should be standardized、add test gates、improve accuracy and stability,Need fuller PR checklists、self-review mechanism、standardize doc and test requirements,Process improvements involve pre-PR checklists、contribution flow governance、doc standard updates、test specs for model or optimizer save load,Need to refine PR submission rules (missing items or weak self-review),Need UI or interaction improvements (e.g. inconsistent Leaderboard layout),Pre-PR checks or code review flow are incomplete,Contributors must fix style issues before submit (e.g. CUDA module normalization and checklist completion),Docs on PR flow or module usage are incomplete,PRs do not follow creation checklist rules,Users ask to clean code style、refine PR review flow、update docs (benchmark and accuracy),Users are unclear how to contribute or follow standard procedures,Sample code paths mismatch refactor causing CI errors,PR flow lacks standard checks resulting in poor code quality,Submodule commit info needs manual update,No standard PR submission flow so docs and code reviews are inconsistent,Submodule updates not auto-synced leading to mismatched refs,Doc update flow is messy and inconsistent causing omissions or errors,Submission process is cumbersome and error-prone,Release steps fail or are not robust,Docs not updated in time so users get confused about CI/CD or features,No notifications on build or test failures so slow response,Community or coverage reports lack discussion stats or focus only on PR diffs,Manual maintenance causes version drift,Submodule commits require manual updates,Need automated submodule commit updates,Automated PR should sync submodule refs,Need automated submodule commit updates,Need version release updates,Need submodule ref synchronization,Need automated PR flows and other CI requirements,Users request automatic submodule commit updating,Automate submodule commits、auto-generate release notes、auto-sync submodule refs,Need automated updates of submodule commits and version numbers,Submodule refs unsynced or need automatic update,Automation needed such as bots updating submodules,Need dev-flow optimizations (CI runtime proxy tweaks submodule sync),Need CI pipeline refresh,Need to define proposal implementation steps,Need to update project version,After update option list valid only on specific branch so CI workflow must adjust,Need submodule sync or retry mechanism,Add or replace tools such as rich for logs and CodeFactor instead of CodeBeat and integrate HF platform,Suggest optimizing CI trigger config、separating doc-only tests、perfecting PR draft norms,Users want automation such as submodule sync、PyPI auto release、CUDA ext auto build,Users want better branch strategy like merging into develop not master,Users demand updated GitHub Actions and CI config,Users suggest improving UI or interaction (e.g. add transparent logo and forum integration),Docs or build flow not automated (doc auto deploy benchmark updates),Need to tidy or squash commits to optimize repo structure,Need fork sync and bot assistance and other automation,Need to host online meetings to gather community feedback,Users propose improving CI tests such as updating allowlist、reducing redundant checkouts、adding local run guides,Users want updated approver list and GitHub workflows and test bot integration,Users request adding doc owners and testing CODEOWNERs file,Users want to trigger bot runs and collect test results via NVIDIA-gha,Users propose creating discussion channels such as Discord for collaboration,Users request updating release or doc branches (gh-pages rel clone guide),Users ask about framework roadmap (main or release branch updates and feature timeline),PR review flow and distributed launch style and log and API doc comments are incomplete"
Community and Document Support,Documentation and Tutorial Support,"Request adding Huawei Ascend NPU installation and usage guide,When memory insufficient should suggest switching ZeRO stage or int8 inference,Clearly describe config field purposes、examples、supported combos such as zero+moe,Clarify whether different models are safe/consistent at batch > 1 and guide deployment settings,For example optimize NVMe performance doc、add assert hints、explain fp16 config meanings,For instance user suggests documenting pip+ccache build-cache usage,User suggests delaying grammar initialization、adding parameter notes、updating xgrammar docs or samples,For example add low-GPU version warning、explain VLLM_RAY_PER_WORKER_GPUS env var,For example suggest extra error print on platform load failure for easier debug,For instance supplement EngineArgs param docs、fix mypy type-check assertions,Neuron environment not detected error unclear、should improve output,For instance auto prompt or supply default template when template missing,AsyncEngineDeadError frequent、user wants more context logs,Doc should note behavioral differences when LoRA enabled,If markdown render fails suggest default HTML fallback or format notice,Docs must state temperature = 0 doesn’t guarantee identical output,Docs should list supported model architectures and quantization methods、detail configs like loadformat、chat method support,For example raise lm-eval tolerance from 0.02 to 0.05 to reduce false fails,Request docs or sample code such as LoRA SFT sample or custom model training guide,README/contrib/training docs need update or extra notes,Official docs lack key scenarios like multi-model training or launch-param config,Need add dist optim doc sidebar/update inference docs,Users request sample of pretraining data format,Need example code for multi-node distributed training and inference,Need release Open-Sora 1.0 model weights and improve docs,Need update open-sora demo docs,Need to update install command docs、README translations、inference module docs,Need add stream-chat sample、auto tokenizer config generation、improve test cases for UX,Need to update or add docs such as missing Japanese translation、tags、checklists,Missing feature descriptions、API docs、user guides or code samples cause difficulty and need standard flow,Need refactor code samples、improve PR norms or naming for maintainability,Doc links dead、explanations incomplete or not updated per spec,Users lack reference implementations like fine-tune or serving samples,Users propose doc additions/revisions such as new support matrix、parallel strategy usage update,Missing example code for models like Llama2 or GPT2,Need update Gemini docs and CI env notes,Request updated examples or docs such as new-feature-ready BERT sample、add docstrings、update PR standard docs,Docs inconsistent with main branch or impl details causing confusion,Shardformer and BERT fine-tune examples missing raising entry barrier,Example code incompatible with new API,Users unclear on feature usage,README or sample doc links broken or style inconsistent,Users unclear how to use new feature or API,Install/use/API docs missing steps and samples causing confusion,Users don’t know how to run script on specific GPU or enable flash attention,Tutorial content outdated or missing so users can’t use features,Lack of example code prevents users implementing certain scenarios,Community model support guide、low-resource training sample、contributor list need supplement or optimisation,Missing dark-mode instructions so users can’t switch theme,Lack of training dataset link prevents reproducibility,No dataset citation or model support hurts credibility,Interface usage complex or inconsistent and hard to grasp,Docs lack conda install or Windows support,No FP8 sample script,No SFT usage sample so users can’t fine-tune,Example missing or redundant code,Reading or usage difficult so users can’t quickly start,Non-English comments or content hard to understand,Users confused about use or config,Sample code maybe mismatched with latest API,Users hard to find and run examples quickly,README/docs lack usage or config instruction,Sample script missing or param mismatch so cannot run,External links 404 or outdated in docs,Project README/sample code incomplete,Users can’t quick-start or guide unclear,Sample demos insufficient or missing,Users can’t get latest info or tutorial incomplete,Docs or configs lack version notes causing confusion or wrong version use,UI elements like web buttons mis-linked or poor UX hinder newcomers,Update FastFold docs and recommender-system URL,Add pytest_wrapper.py module docs,Need update GitHub project docs,Need update ProcessGroup class docstrings,Need add ColoTensor usage docs,Users request more code samples、README updates、added comments etc doc improvements,Fix README and Dockerfile、enhance ColoTensor debug such as __repr__ method,log_to_file function and chunk.py lack usage notes,Need add Colab tutorial and Bert test cases,Improve doc readability such as communication module/amp docs beautify、type-hint enhancement,Logs miss TFLOPS metric need recording,Record TFLOPS metric,Chinese and English docs unsynchronized,README missing key info,Lack project integration tutorial,Install commands outdated,Need add external resource links,Missing Swish Transformer sample,Users want reinforcement learning or low-resource samples,Docs incomplete or missing key content like parallel model definition、checkpoint explanation,Need refactor/update API docs or docstrings,Need update contribution guide、README paths、module docs such as ZeRO updates to match current flow or fix errors,Need integrate config tutorial、simplify param hook registration、add model init sharding explanation,Need add contributor badges、improve issue template content、update README,Users request extra Chinese docs、tutorial explanations or experiment result displays,Users note PyPI package lacks metadata such as long description,Need update or fix docs like README、API docs、tutorial files、AMP model docs,Users request adding or integrating model samples such as GPT2、GPT3、deeplabv3+、CamVid dataset sample,Need adjust logging config such as default log name change、disable conflicting logger,Need update API docs/Sphinx docs/English markdown and add Chinese support,Missing application samples like MNIST classification、ViT-B16、SimCLR self-supervised、hyperspectral processing,Users suggest optimizing doc organization such as rearranging parallel-method order,Users need environment variable to control doc generation language,Need supply sample code or more complete usage docs,Missing promptTuningEnabled param description,README lacks libnccl2 install notes,Users request missing “Serving” docs、improve existing docs wording、add contributor guide,Need add API samples、model support description or update main README,Need fix docs not using alert formatting correctly,Users request update or add doc content such as README.md、support matrix、API stability notes,Users want easier usability such as simplified API calls、template gen guide、better logging,Users request updating contribution guide、license comments、roadmap,Interface replacement such as GptManager to Executor confuses users、needs explanatory docs or samples,README or docs outdated/missing such as DeepSeek V3 notes、Speculative decoding config,Users request update or add project docs such as linux.md、disaggregated-service.md、gh-pages branch,Users need more detailed docs or guides such as BERT inference、C++ samples、API reference page update,Users point out doc errors or missing info such as BF16 support notes、copy-code widget missing,Users ask how to add new model or feature and lack related docs or sample guidance,Users request doc updates such as gh-pages、quick-start guide,Users ask open-source plan for modules such as batch_manager or executor,Docs incomplete or outdated such as Windows section、GitHub image links、release info,Users ask how to use functions such as enable_block_reuse、kv-reuse or custom position_ids,Users need docs or samples such as README enhancements、API migration guide、System Prompt Caching usage,Users request adding AMMO toolbox docs、fix README links、explain inflight batching support、nitro-tensorrt-llm project notes,Users ask for model integration guides such as Mistral+ViT、engine param config、LoRA adaptation plan、custom network construction,Users request sample code or doc guides such as C++ interface sample、branch clone guide、feature usage tutorial,Users need docs such as KV cache config guide、prompt input support notes、parallel deployment sample,Users hope framework open more implementation details such as CUDA code,Users request updating docs、adding samples or diagrams explaining framework components,Users request adding “Latest News” module or sample code,Users ask about param differences such as max_batch_size vs max_num_sequences、performance comparison methods、model build communication mode,Users say README and other docs need readability update,Users ask if BatchManager will stay closed source or future open plans,Users request update Windows docs to main branch,Lack internal API call explanation docs,Need new multi-node multi-GPU training guide,Need adding fine-tune training sample,Need additional param explanations/training guide,Users need single-card vs distributed inference steps and differences,Config files (yaml) lack clear categories and purpose notes,Users need concrete deployment guide for models such as qwen2 series,Users need RLHF fine-tune method guide,There are model file confusions like llama/llama2 and missing operation guide,Missing config params and samples cause inability to understand or operate,YAML/JSON config naming differences or docs inconsistent with scripts causing load failure or confusion,Users point out homepage needs update fixing “can’t use LoRA、FP8、MC2 fused compute、CosyVoice etc or can’t run on Megatron Core 0.7.0”"
