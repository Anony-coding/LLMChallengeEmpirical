rootcause,subcategory,symptoms
System Compatibility and Integration Failures,"Build、 Installation、 and Low-Level Support Failures","Compilation failure、build error、platform not supported,Extension module build failure、missing library or link error,Pre-build failure、compilation error or missing dependency library,Unable to build wheel、missing compiler or incompatible compilation flags,CUDA extension or CUTLASS path not found,Installation failure、path configuration error、import migration failure,Code build error (such as simd_load missing parenthesis、hipify failure),Build failure (missing dependency or path error)、CUDA component missing,Missing library files、build script error、header file not declared,Missing ROCm library in build,Cuda library missing,Module import failure,Installation/compilation failure or missing header file、undefined symbol、compilation error,Project addition failed due to unsupported PEP 517,Missing cuda header files、__double2half and other symbols、build failure or inference error,cl.exe error or build failure,Failed to build successfully、installation failure、error undefined symbol or compile error,Missing some libraries or symbols causing ImportError/ModuleNotFoundError,Installation failure or runtime error,Import failure or attribute missing (such as torch.compiler.is_compiling),Compilation error or runtime interruption,Failed to extract git information or build failure,Unable to build or install,Installation failure、import error、compilation failure,Benchmark folder mistakenly included"
System Compatibility and Integration Failures,Operator Compilation and Backend Support Failures,"Missing extension、Triton incompatible with CUDA、build error,hipSuccess failure during build,Runtime compilation failure"
System Compatibility and Integration Failures,Versioning and Compatibility Breakages,"Inconsistent methods/types between old and new torch versions causing compilation or test failures,Triton 2.1 API changes,Torch nightly incompatible,DeepSpeed version mismatch with Pydantic、Cupy、Apex、Pytorch and other libraries leading to build failure or runtime crash,Pytorch、Torch version check failure、libaio、triton missing、Windows build failure,Transformers or torch version change causing functional anomaly,dlpack、triton、libuv、transformers upgrade causing runtime or build failure,New versions of Triton、Torch >=2.3、transformers、cuda causing errors"
System Compatibility and Integration Failures,Environment Dependency and Toolchain Compatibility Bugs,"PMI initialization failure、PMIX spelling error or env error,CUDA string hardcoded、environment variables not suitable for NPU or device usage error,Setting CUDA_VISIBLE_DEVICES causes invisible or incorrect device usage,Missing dependencies、Docker or Windows installation failure,OpenMPI interface hardcoded、incompatible with torch 2.0,Bash shebang writing、path handling、delete command incompatibility causing installation failure"
System Compatibility and Integration Failures,Hardware and Accelerator Adaptation Failures,"HPU backend incompatible with default backend causing test_set_compiler_fn error,Unable to locate GPU device handle、error during training initialization stage,Inference or training error or performance anomaly due to incomplete hardware support、functions undefined or data error in MPS、ROCm、NPU,XPU/NPU/MPS/HPU/ROCm and other non-NVIDIA platforms not recognized、inference failure or missing key ops,Training/inference failure on AMD/XPU/HPU/NPU and other non-NVIDIA platforms,Error at program startup unable to recognize GPU topology,CPU inference failure、tensor cross-device conflict,rocblas include missing、NPU UT skipped、HIP error,ROCm/HIP/NPU platform function failure or build error,MPS error、4090 GPU deadlock、regex cannot be parsed on Mac,CPU parameter mismatched with CUDA,Build failure、unable to use specific hardware,Building DeepSpeed on different platforms (such as M1/M2、MI200、ROCm) causes nvcc error or intrinsic missing,MI300X GPU training error"
System Compatibility and Integration Failures,Platform and Deployment Bugs,"Build or installation failure in Databricks、Windows、WSL、ROCm environments,Unable to build/run、prompt for missing platform-specific library or abnormal behavior,Compilation or runtime failure in Windows、ROCm、Pydantic 2.5 and other environments,ROCm、Windows environment build failure or feature unavailable,Incompatible with ROCm/K8S/NPU and other platforms、version conflict,Build error or path incompatibility on Windows 11、ROCm、AMD GPU and other platforms,Build failure or module not found on Mac/Windows/AMD platform or Python version,os.rename fails on Windows,Windows build failure,GPU detection failure in Docker image、NVML initialization failure、libaio configuration anomaly,Windows does not support GDS、MacOS hostname error、Apple Silicon incompatible,Running failure or missing feature in Windows、WSL2、PowerLinux、Ascend 910B and other platforms,Build/run failure in Windows、ROCm、XPU、MPI、Docker and other environments"
Resource Efficiency and Memory Management,Scheduling and Parameter Management Bugs,"Task imbalance across devices causes some devices to OOM or have extremely low utilization,Insufficient GPU memory、OOM or improper batch size setting during training,lp grads not cleared or improperly partitioned、leading to increased memory usage or training failure,Overflow or runtime exception occurs during offload,Incorrect training resource estimation、leading to OOM or inefficiency,KV cache state reused incorrectly or fp16 weights not cleaned properly,OOM、memory leak or backward time grows linearly,Repeated model loading during inference causes continual memory increase or overflow,Redundant copies or CUDA initialization failure,Memory leak、forward hook mis-triggered、shard loading failure,Offload training or inference failure、OOM、memory not released,CPU memory spikes during Zero2/3/Infinity checkpoint、gradients not cleared,Multi-GPU inference unresponsive、Zero3 OOM、inference terminated abnormally,Lock file release abnormal、hooks not released causing memory leak,Under CPU migration strategy GPU memory not released、frequent OOM,After enabling CPU offload CUDA memory remains high、conflict crash,Pipeline + accumulation causes CUDA OOM、communication desynchronization,Memory not correctly released、checkpoint buffer error,GPU memory usage far exceeds expectation、gradients not properly released,OOM during training、logits diverge、process hang,OOM、memory leak,Parameters not released,Zero3 GPU memory usage abnormal、checkpoint restoration incomplete、grad_bias calculation error、parameter states inconsistent,Memory leak、process OOM、repeated task runs exhaust resources,Uncontrollable memory growth during training (such as contiguous gradients),CUDA OOM occurs during training or inference、training failure、generation error,Frequent CUDA OOM triggers、memory leak or abnormal memory allocation during training or inference,Frequent OOM、illegal memory access or unreasonable memory configuration during training or inference、causing crash or performance degradation,Parameters not initialized、memory leak、duplicate data pointers、stage transition error causing training failure,NoneType error or abnormal termination when all experiments OOM,Memory not released、gradients not collected or behavior deviates from expectation,Main process memory continuously grows,OOM、memory leak、NCCL communication crash、unexpected resource binding during training or inference,Frequent OOM or illegal access errors in multi-GPU or large model training,Causes memory leak or unstable testing,Leads to insufficient GPU memory or memory overflow,clone or grad function not releasing resources causing GPU or CPU memory growth,Memory overflow during model initialization,Buffer overlap、repeated allocation、not released causes GPU memory overflow (OOM) or temp_buf conflicts with output,CPU/NVME offload state confusion leads to loss not decreasing or parameter position error,OOM error / high GPU memory usage / GPU unresponsive,GPU memory overflow (OOM) but monitoring shows no anomaly,Still OOM when training large models (e.g. T5、OPT) with DeepSpeed；memory exhausted during inference,CUDA memory overflow during MoE model training,OOM (memory overflow) during inference causes server crash"
Resource Efficiency and Memory Management,Performance Optimization and Monitoring Failures,"Increased processes affect training efficiency、low GPU utilization,Zero inference is slow、GPU utilization is low、poor results,Low GPU utilization、training speed drops、memory overflow or OOM,Memory optimization ineffective or increases abnormally,cudaMemcpyAsync takes too long、reduce_bucket_size is unreasonable,Offload performance degrades or crashes、NVMe checkpoint error,Torch allocator inaccurate、GPU utilization uneven,ZeRO3 consumes more memory than Stage2,Model training slows down、memory usage unbalanced、communication hangs,Such as statistics method failure、large model CPU loading exhausts memory、too many reduce operations,Only half of CPU cores used,Gradient checkpoint uses excessive memory、GPUUtil stays at low utilization,Zero2&Zero3 configuration ineffective causing memory not to decrease,GPU count increases but memory does not decrease、CPU offload ineffective、inference performance decreases instead,Inference speed slows down,Slow inference speed,Inference performance decreases or latency increases"
Resource Efficiency and Memory Management,Quantization Accuracy and Inference Performance Bugs,"8bit dequantization performance poor、bf16 overflow check missing,Using FP16/BF16 causes GPU memory computation error or memory leak,Quantized model insufficient CUDA memory,Extremely slow speed or excessive latency during quantization processing"
Distributed and Parallel Execution,Distributed Communication and Initialization Bugs,"Multi-node or distributed training error、training hangs or timeout,Incorrect world size、node communication failure,Resources not correctly recognized in multi-GPU or multi-node training、scheduling failure or OOM,Assertion failure or training stall when setting global variable/mesh device/world size,NCCL timeout、ZeRO partition issue or fusion failure,Process rank anomaly or device error,Model sharding configuration error、Zero stage parameters inconsistent、allgather overlap ineffective,Connection timed out、MPI parameters unset、multi-node training failed,Intel MPI binding conflicts with launcher configuration,NCCL error during training,Multi-node training failed,world size setting error,Inconsistent rank,Unequal GPU counts in multi-node training causing rank error,MPICH launcher failure、RANK calculation error、CUDA_VISIBLE_DEVICES setting ineffective、worker count inconsistent with config、RANK calculation error,Unequal GPU numbers across nodes causing rank mismatch or load anomaly,Issues such as ZeRO3、distributed initialization、rank mapping cause training hang or launch failure,.id attribute assigned repeatedly causing GPU placement error,Non-master node mistakenly calls init leading to multi-node hang"
Distributed and Parallel Execution,Distributed/Multi-Process Data Communication Failures,"Cross-device tensor misaligned causing RuntimeError during inference,torch.stack or norm across CPU/GPU fails causing training anomaly,Loss zero in ZeRO3 training、evaluation accuracy zero or training hangs,Some rank parameters are 0 causing training failure or error,Communication steps such as grad norm、reduce scatter fail,Some ranks missing parameters、pipelining error or assertion failure,Gradients unbalanced、communication operation failure、data misaligned or communication timeout,Gradients not merged correctly、communication logic error or partition error、leading to model not converging,Communication state abnormal or parameters not synchronized correctly、leading to distributed training failure,Communication timeout during training,all_gather causes extra GPU allocation、optimization suggested,Activation misaligned,Tensor broadcast duplicated,Model cannot load、weights missing、RuntimeError、gradients unsynchronized,Error when calling _allgather_params,Multi-GPU training failed、deadlock or data inconsistency"
Distributed and Parallel Execution,Communication and Coordination Errors,"Lack of synchronization or fixed seed causes inconsistent test results,Large differences in training results across GPUs、suspect scheduling strategy influence,Batch allocation error in PP+DP mode,Training hangs、distribution uneven、micro-batch empty,Reduce order disorder、mapreduce concurrent write failure,Communication failure、synchronization error、pipeline epochs inconsistent,Multi-GPU slower than single GPU、inference values inconsistent,Communication deadlock,Partition imbalance,Multi-GPU performance anomaly,Pipeline rank resources uneven,In Megatron SP mode each TP rank receives identical data,Multi-GPU output duplicated or inconsistent,Communication logic errors (e.g., barrier、broadcast) in Pipeline/ZeRO parallel cause hang or desynchronization,Device bubble occurs or synchronization wait too long,Process hangs during inference or training,Some GPUs finish then wait for others causing latency,Redundant all_reduce leads to deadlock,Inference hangs or process blocked"
Numerical Stability and Precision Management,Numerical Computation and Operator Stability Bugs,"Dimension error in forward pass or “Sizes of tensors must match”,grad_buf dtype mismatch causes numerical error,Warning still appears in non-fp16 mode、users unclear about actual status,Enabling overlap_comm in ZeRO Stage 2 leads to inconsistent loss,Loss overflow、optimizer configuration or checkpoint message ambiguous or misleading,Such as allocate_workspace_fp16、replace_module causing TypeError/NotImplementedError,Changing HF layer strategy makes logits inconsistent,Optimizer creation log fails to show bf16 support、misleads dtype information,Continuous log printing、accumulated step-count estimation error,Gradient-update frequency miscalculated,Causes some weights to be unallocated,seq = 4096 makes softmax_context_func divide-by-zero or overflow,Error / precision issue / synchronization problem,FP16 kernel incompatible with Pascal architecture causes crash,Special sequence length triggers RuntimeError or accuracy drop,CUDA_HOME missing、header not found、fp16/fp32 kernel compilation fails,Output becomes all-zero vector / training fails / loss does not decrease,Gradient-partition order inconsistent、empty gradients unhandled、round-robin error、accumulation-logic flaw,Activation checkpoint lowers accuracy、loss display wrong under gradient_accumulation_steps、mask parameters untrained,ZeRO-1/2/3 training logic inconsistent、loss mismatch、config ineffective or crash risk,Mask error、FP16 returns nan、Triton compatibility issue,FP16 softmax misuse、model outputs inconsistent、seq2seq inference error、model state mismatch after load,Inference-API error、FP16 half-precision inference abnormal、GPT-Neo load failure,Loss curves differ greatly across stages,OneBitAdam incompatible with ZeRO2、ZeRO-1 poor effect、gradients not default True,Stage 3 dimension error,Torch.cdist fp16 not implemented,log(0),Dummy overflow-check logic wrong in float16 mode,CUDA kernel index type error,FlashInfer v0.2.3 sampling error,Top-k gather index type error,Numerical error on WSL2,Large-tensor transpose causes performance issue,Partition.process_json_file uses undefined variable key,Cross_entropy under tensor parallel differs by 1 %,CUDA Graph combined with 1F1B causes NaN gradients,Loss inconsistent inside CUDA Graph environment,Accuracy inconsistent in PP,Embedding output precision abnormal,FA3 accuracy drops,Logits inconsistent,Transformers and vLLM inference results differ,Profiling causes measurement bias,Divide-by-zero error in logprobs,Divisor 0 triggers exception,Hessian cannot be inverted,test_fused_moe tolerance insufficient,metrics computation divisor error,RoPE configuration abnormal,Model accuracy lower than expected,OCR accuracy drops sharply after v0.6.2 upgrade,Results differ from OpenCompass,Model generation quality degrades,Triton assertion failure under FP32,Qwen2-VL-7B-Instruct accuracy decline,Phi-MoE model outputs strange characters,Different results when using awq model parameters,Llama3 RoPE init needs refactor、Phi position-embedding overflow error,rope scaling unsupported causes Gemma2-9b run failure,TP_size divisibility issue causes tensor-size mismatch、precision-conversion issue (bf16 support)、Perf conflicts with Transformers version,FP16 negative-split error、MoE forward precision overflow,Operator implementation defect (e.g., FP8 all-reduce algorithm error)、kernel compatibility insufficient (e.g., flash-attention missing lazyload),Inference results differ each time,Data-type mismatch in element-wise addition,Model accuracy drops or build fails,Python and C++ runtime results differ,Same input produces different results multiple times,Inference results differ with different batch sizes,Output abnormal or NaN,Result differences across versions/parameters,Long-text input/output accuracy degrades,MMLU score abnormal,int32 index overflow causes build failure,Parameter value error causes numerical processing issue,Head size or row/column byte count invalid triggers assertion,Conv2D calculation shows precision difference,User reports numerical inconsistency (e.g., modified QWenAttention precision difference、int8_sq_per_tensor mode anomaly),nan values or loss divergence"
Numerical Stability and Precision Management,Precision and Quantization Error Management Bugs,"BF16/FP16 tensor handling error or slow convergence,4-bit/8-bit/fp6 quantized model type error or unsupported at inference,Inference failure under low-precision computing (e.g., bf16/fp16) 、precision error or mixing fp32 causes crash,Gradient/precision/weight-loading errors when combined with MoQ、4-bit PTQ、ZeroQuant,Gradient overflow、loss chaos、optimizer incompatibility、precision error when using bf16/fp16,Loss scaling incorrect、log overflow handling chaotic、behaviors differ across precisions,Low-precision models fail or deviate greatly on GPT-J、OPT、Bloom,Such as int8+GPTJ、bf16+GPT2、mixed dtype + ViT,pydantic configuration lacks bf16 support,int8 load failure、precision loss、inference slower instead of faster,Improper memory access、stride、type casting in quantization kernel causes precision issue or crash,MoE sum kernel loses bfloat16 precision when topk = 8,OPEA quantization yields abnormal results,bfloat16 causes overflow,fp8 type handling failure,int8 precision slower than fp8,bfloat16 and float32 used with wrong formats,Adding bfloat16 support triggers test failure,Qwen2.5-VL-3B float16 overflow,Precision-related sampling error,flashinfer version change impacts fp8 accuracy,4-bit inflight quantization parameter error,FP8 KV cache warning,Attention quantization abnormal,bnb prequantification error,INT8 Qwen2.5 generates meaningless sentences,int8-A8W8 unstable,llama3 int8 inference error,Zero-point folding error in static symmetric quantization,INT8 weights too small cause test failure,convert_fp8 uses wrong type,FP8 quantization crashes on AMD MI300X,FP8 model numerical error,AssertionError under GGUF quantization,Float16 inference fails when chunked prefill enabled,Meta-LLaMA FP8 test failure,AWQ failure、legacy quantization precision hurts glm4v、fp8 cache incompatible with Ampere,FP8/AWQ quantization leads to core dump or accuracy issue,Parameter mishandling during quantization causes abnormal output (e.g., redundant ops in INT8 quantization),int4 weight-only engine outputs garbage characters,Output empty when using smoothquant,Quantization failure,FP8 quantization causes repeated inference results or attention computation failure,W4A(FP)8 quantization lacks zero-point check causing error,INT8/INT4 failure error,Error during quantization causes conversion or inference failure,Engine build or inference abnormal under specific precision mode,Quantization failure or wrong output,bfloat16 build failure,TypeError during quantization process,INT8 calibration failure,Missing scale and zero-point warning,Quantization process failed or export type error,int8_kv_cache or int4-awq causes precision drop or build error、linear-layer fusion affects quantization accuracy,AWQ/Int8 quantization function error causes model performance drop or build failure,Improper INT8 quantization range causes precision loss or build failure,Quantization with SLora affects network precision,Post-quantization error or inference latency/precision abnormal,Numerical split failure during quantized inference"
Numerical Stability and Precision Management,Checkpoint Precision Compatibility Failures,"Checkpoint supports only str,Model converted to ckpt yields abnormal inference,Numeric error makes checkpoint file unsupported"
Training and Inference Control Logic bugs,Training Strategy and Optimizer Usage Bugs,"Unable to optimize only a specific sub-network、or Adam Offload throws an error when mixing CPU/CUDA,Learning-rate scheduler not effective or initialized with wrong value,Parameter groups merged、learning rates not isolated、fused_adam failed to load,FLOPs underestimated,Scheduler does not decay,lr scheduler not saved,Training logic error"
Training and Inference Control Logic bugs,Model Training Stability Failures,"Training stalls or precision error during training,Training stagnates、evaluation accuracy is 0、loss is 0 or not updating,Training loss shows abnormal periodic pattern,Gradients zero during training、loss abnormal or save fails,Loss oscillates cyclically、does not converge or gradient abnormal,Training crashes、worse convergence、abnormal inference outputs,Training process stalls、start_profile fails or training hangs,Ctrl+C kills all training tasks,Trainer integration deadlock、PPO loss fluctuations、scheduler missing,Loss stuck at 0、uncleared initialization causes IndexError、gradient None,Training freeze、excessive recursion causes error,Alpaca loss abnormal,Loss is 0,HybridEngine training unstable,Loss is 0、experts not invoked、checkpoint load failure,Hang during training or inference、random failures or inconsistent results"
Training and Inference Control Logic bugs,Initialization and State Configuration Anomalies,"wrapper attribute in CompiledModuleWrapper overridden、forward not called,Long stall during training initialization phase,CPU/GPU optimizer behaviors differ or initialization inconsistent,Checkpoint load failure、parameter mismatch or weights not loaded,Multiple initializes cause state corruption,Model behaves abnormally after restore such as weights not loaded or training from scratch,Variable access failure or unhandled real_accelerator fallback,Logic flaw in checkpointable_layers、quantized weights not reset,Index error after freezing weights、initialization conflict causes IndexError,init_inference failure、multiple engine conflicts、bias becomes NaN,Meta tensor migration issue、checkpoint layer name mismatch,State dict naming conflict、nonreentrant checkpoint replacement failure,module_state_dict frozen-parameter KeyError、checkpoint cannot be restored,Improper device selection,Port conflict,Weights not synchronized,Key mismatch,State not synchronized,Parameter count is 0,AttributeError at runtime or parameters on different devices"
Training and Inference Control Logic bugs,Optimizer and Gradient Strategy Errors,"Layer with requires_grad=False prevents release logic during backward,grad_norm miscalculation causes model not to converge,Gradients None during training、optimizer state wrong、checkpoint load failure,Error in allreduce or forward、or gradients are None,grad is None during training causing assertion failure or crash,Some parameters no gradient、training hangs、accuracy drops,Gradients inconsistent、state not loaded、update failure or gradient update bias,Gradient clipping ineffective、loss becomes NaN or LR scheduling chaos,grad norm is NaN、loss is 0 or parameter state fetch abnormal,Gradient accumulation abnormal during training or model parameters cannot be restored,Assertion failure or optimizer state cannot be restored,zero_to_fp32 tool logic error,Second backward error、nonreentrant checkpoint ineffective,optimizer_mem update ineffective、TFLOPS fetch error,Gradient checkpoint conflicts with zero、grad is None,zero_grad inconsistent with torch、bf16 optimizer error,loss scale is 0,Optimizer not configured or weights empty,Parameter group error,Optimizer gradients error,tie weights handling error,Optimizer not synchronized"
Training and Inference Control Logic bugs,Model Unresponsiveness or Erroneous Inference Outputs,"Pointer or shape error causes inference RuntimeError,Error during inference、repeated outputs、performance degradation,Output tensors duplicate、slow or not converging during inference,Inference performance inconsistent、hangs or result bias,Repeated outputs or wrong memory in multi-round inference,Causes crash、None data or deadlock,Failure to enter model.generate() causes training stall、GPU freeze,AttributeError,Input exceeds max_tokens causing segfault,assert error,Generation behavior failed,attention mask abnormal,Output inconsistent、inference abnormal、some functions unimplemented"
Data Pipeline and IO Management,Data Flow Processing Failures,"Batch size error、None tensor、deadlock、zero file,Abnormal batch size、deadlock or DataLoader crash,Dataset count mismatch causes deadlock、communication-config conflict,input/target not passed to model engine correctly causing error,Long input、long output、large batch or GPU distribution issue triggers CUDA OOM,Identical input yields different output、random behavior anomaly,Last batch size abnormal or error,Batch size mismatch、register_buffer missing content、torch DataLoader misuse,Training-step display error、uncontrollable remaining batches,Value modified during data transfer、eval output anomaly,Disaggregated Prefilling error,Finetuning with microbatches>1 unsupported for gradient accumulation,dataloader worker intercepts SIGTERM causing exit-handler failure,Response leak,Missing multimodal responses、multimodal tests fail,Request disconnection unhandled,Streaming did not return complete result,stream generate failed,Client terminated without canceling request,Request not aborted when client process ended,request.is_disconnected cannot report disconnect status、request non interruptible,Request exception non interruptible,Streaming result is None,Request non abortable,embeddings endpoint unresponsive,Streaming response returns only one token,Non streaming API cannot correctly produce usage info,Prometheus /metrics returns nothing、/metrics outputs nothing,Data loader cannot form valid batch or data format error,Loss computation exception triggered while dataloader fetching data,GradAccumDataloader skips last iteration、dataloader mis handles data、data format mismatch,Streaming response support missing/parameter pass error/cache manager null pointer,Triton endpoint lacks decoupled transaction policy/streaming data loss,Streaming output callback not invoked as expected,aclnnCast call failed possible network or data transfer issue,Multi node data cache service error,Data loading or preprocessing failure causes training error,Error triggered during dataset training"
Data Pipeline and IO Management,Data Reading and Input Errors,"Data not ready when init called,Path misconfiguration or cache policy bug,Error running example or test script such as missing file or wrong path,Failed to fetch URL or token,Duplicate test name、high I O load or unchecked device count causes CI hang or fail,IO warning printed when NVMe not set,Dataset type undefined limiting training,dataset config parameter conflict causes crash,Tokenizer file existence unchecked causes exception,Missing dummy tokenizer,S3 load failure,Visual data access failure,Early return in preprocess_data.py prevents bin or idx creation,Missing BookCorpus dataset,JSON decode error when building tokenizer,Qwen2.5-VL service lag or image request error,Whisper test failed or Whisper offline function abnormal,Qwen2Tokenizer error,sentence_bert_config.json returns 404,openai API returns empty string,ModelScope load failure,Huggingface hub HFValidationError,Unable to copy file,File path or IO exception such as FileNotFoundError,Missing correct path prevents module import,Missing required file or module causes feature failure,Include path misconfiguration causes compile or import failure,IndexError loading dataset with few_shot disabled,Missing required files or modules such as utils.py or accelerator,Data preparation error interrupts workflow,AssertionError due to missing data column text,Repeated reads from same source cause faulty behavior,prompt_dataloader duplicate read、seed dataset conversion error、KL divergence sampling source error,Dataset download link、pretrained weights or config file missing,Cannot find file or wrong directory chosen,File or directory missing causes run interruption,Path setting issue causes feature failure such as glm-4-9b path,Path change breaks dependencies such as requirements.txt lost or model path misconfig,Server returns non 200 status such as HTTP 500 during benchmarking,Exception during load,Missing config or weights prevents script execution,File generation error,HTTP request handling stuck or batch request fails,Cannot load plan file or rouge.py missing,Engine path misconfiguration、dataset missing breaks benchmark、weights unbound causes build error,Shard download stuck,Dataset load failure or preprocessing error,File path registration error or strategy file missing,Dataset register path wrong causes processing failure,Wrong file path causes resource load failure,YAML file not matching run script path,File missing during model prediction,Missing config.json file,Missing predict_glm2_6b.yaml,wget downloads only part of data,Missing config file such as config.json or tokenizer.json causes task abort,alpaca_data.json missing so inaccessible,Network failure during model download,Single host multi GPU inference error missing dataset file,Prediction fails due to wrong file path,Dataset conversion error such as tik import fail,Wrong path prevents file load or assertion fails,Dataset link invalid or input data shape mismatch,Dataset processing or tokenizer missing causes failure,Dataset creation with read_function parameter fails"
Data Pipeline and IO Management,Data Preprocessing and Type Handling Bugs,"Tensor shape mismatch when Pipeline Parallel inputs differ,metric_type accumulation error、collate_fn None triggers exception,Wrong shuffle setting degrades convergence,Enabling multiple options LORA or gradient_checkpointing causes AssertionError,Option combo unchecked、AssertionError or semantic error gradient_checkpointing with only_lora,NamedTuple output、tensor dimension error、multiple tensor input causes assertion fail,Softmax input mismatch、Bloom attention mask size mismatch、over long tensor input causes RuntimeError,steps_per_print ignored、input shape cannot update、dtype force check fail,GPT or OPT or Roberta or NeoX inference wrong or inconsistent with batch>1 or long input,lcm greater than max_acceptable_batch_size fails test,Some matmul or einsum operations not shown,max_train_batch_size unspecified leads to None or doc error,Length calculation error、drop_last missing leaves residual data,Matmul not logged、packed RNN unsupported、einsum error etc.,Multiple video input triggers data structure error,Dataset preprocessor logic error,Dataset list config triggers blendable_dataset.py error,JSON multiple key or loop not finalize all keys causes parse fail,Sampling probability config abnormal in BlendableDataset,Input processor causes mllama error,Chinese text batch processing fails,multi modal input tested with limit=1 only,do_lower_case causes UNK,Qwen-VL-Chat OOV error,Multimodal tokens mismatch placeholders,PixtralHF non standard image resolution fail,Only one image in batch unhandled,hermes_parser JSON parse fail,XGrammar empty output,sampling_metadata copy omitted,JSON dict loses order using prompt_logprobs causing token order error,Hermes parse stream error,MistralTokenizer fails to filter special tokens,tokenizer_mode missing causes hang,Over long token in streaming mode fails,Mistral tokenizer lacks id_to_byte_piece,Qwen2-VL mishandles mm_processor_kwargs causing vision input inference fail,Mistral Tekken Tokenizer special token crash,Molmo dummy data validation fail,Hidden States retrieval fail,spec decode cannot access hidden_states,Qwen2VL image causes scrambled output,Error when more than one input image,Image placeholder wrong,Image input not parsed,Placeholder error,mistralaiCodestral truncated,MistralTokenizer or CachedChatGLM4Tokenizer missing vocab attribute causes AttributeError,Wx1 image dimension input triggers transformers exception,Qwen2VL image embedding dimension mismatch triggers error,Handling 1×1 or no placeholder image triggers channel error or crash or UnboundLocalError,benchmark input hints unsynced,Tokenization differs from HF,JSONDecodeError,tool_parser error affects streaming chat,MistralTokenizer non UTF decode fail,Detokenization encoding error causes crash,Token versus placeholder mismatch,get_vocab attribute missing,Image handling needs manual token padding,MistralTokenizer lacks get_vocab,Pipeline generator escape character error,guided generation JSON incomplete,Tokenizer decode logic exponential growth,Image preprocessing called repeatedly,Shared prev_tokens in detokenizer causes exponential growth,Multimodal input placeholder error,API returns garbled words,InternVL multi image I O garbled,VLM placeholder out of context causes error,JSON output inconsistent,image_idx not passed correctly leads to inference fail,llavanext batch inference image tags mismatch placeholders、API EndOfStream error,Data type or tokenizer issue causes calculation error such as embedding calculation error,Cannot handle specific data type BatchEncoding,Dataloader length wrong、dataset file mis chosen、preprocess label assign error"
Data Pipeline and IO Management,Model Persistence and Saving Failures,"Incomplete save or load failure,state_dict param redundant、freeze weights fail、nested Init causes recursion error,Error in save_state phase,Cannot correctly save or read checkpoint,Offline inference cannot complete,Chat cache file causes exception needs removal,state_dict save or load error,Unable to export intermediate values or export exception"
Data Pipeline and IO Management,Model File and Checkpoint Compatibility Bugs,"Cannot merge checkpoints in multi node training,Checkpoint with PipeEngine or CompiledWrapper reports error,Different GPU count in checkpoint causes restore failure,Checkpoint weight load failure,Zero3 checkpoint load error,Weight loading failure,Missing key,strict param missing in load_state_dict causes restore fail,Sharded checkpoints will not load、weight attribute missing、scale wrong、shape or index mismatch、init fail,Parameter sharing ineffective or wrong weight load,state_dict size mismatch、restore fail、some weights None,Model restore fail、some weights missing、error,Weight sharing or restore logic error with LoRA etc. causes inconsistent outputs,zero_to_fp32 or checkpoint file structure error causes restore fail or size bloat,ZeRO3 does not retain frozen weights、checkpoint state incomplete、Embedding unusable after restore,Model list corrupted or load fail,Model state inconsistent after checkpoint load,Model load failure、state_dict mismatch,load_checkpoint cannot read saved result,GGUF file without extension unrecognized,No .gguf extension cannot load,safetensors weight file not found,Model file incomplete download,File not found or wrong format such as config.json path error or gptq format issue,Config or model path invalid JSON parse fail or pretrained model load fail,Model file download interrupted or corrupted causes train or infer fail,convert_checkpoint.py conversion script failed,Converting InternVL2-1B etc. fails due to script or version issue,AutoConfig cannot load HF config causing error,ONNX model load fails missing field declaration,Qwen2VL model conversion failed,Model name mapping error causes load fail,convert_checkpoint.py crashes during conversion,Conversion script failed,convert_checkpoint.py environment or config error causes conversion fail,InvalidHeaderDeserialization exception,InferenceRequest object not serializable TypeError,Error locating weights or data file,Weight download failure or file missing,Single safetensors file checksum fail,Model inference weight load failure"
Interface Design and Configuration Management,API Interface and Parameter Configuration Errors,"set_none_gradients_to_zero misspelled or scale value wrong,Using CUDA default interface incorrectly to create tensor causes device crash,Argument position wrong、quotes mismatch、state_dict misuse,Parameters not assigned to correct device or cause model init exception,Model expects cuda but allocated on cpu、load fails,pipeline module did not pass strict parameter,pipeline module error,Head count mismatched with parallel setting causing assert fail,KeyError when calling initialize,safe_set_full_grad、offload_states and similar interfaces failed or had no effect,Passing illegal type to kernel inject parameter triggers TypeError,bias_gelu and similar module tests fail,Model cannot load / OOM / parameters unsynced,Corrupted output during inference、last batch output wrong、progress bar not updated、rank config error etc,Some parameters not cast back to FP32、layer missing or order wrong,torch.add and similar patch method parameters unhandled causing crash,`step()` interrupted / parameter error,Speech or image task initialization error,OneCycleLR default config triggers ZeroDivisionError,zero_to_fp32 weight extraction wrong / parameter misaligned,Parameter group missing field error,Unused parameter assertion enabled by default,get_cpu_offload_context parameter passed wrong,AutoProcessor load failure,Missing optimizer field causes crash,router_dtype parameter missing,vocabfile/mergefile parameters not handled,QKV bias attribute missing,optimizer is None causing model save error,init_method not declared,get_cpu_offload_context parameter wrong,transformer_engine.py conditional parameter count wrong,HF checkpoint attribute missing,get_cpu_offload_context parameter error,vocab.json missing,BadRequest error output,Output garbled after tool selection,invalid tool param,test_sleep parameter wrong,Benchmark init parameter wrong,Returns 415 status code,ChatCompletionRequest rejects default values,best_of=1 wrongly rejected,sampler parameter None causes error,Auto tool invocation missing parser,V1 returns wrong response count,offline inference example missing parameter,OpenVINO backend parameter missing,Empty tool field causes Llama template failure,skip_special_tokens parameter crash,Cannot set kv cache dtype,cache_full_blocks parameter wrong,Client status not reported,transcriptions endpoint signature fail,bad request request fail,POST signature fail,device_id_to_physical_device_id parameter cannot cast to int,function call parameter None error,test_sleep parameter wrong,Benchmark init parameter wrong"
Interface Design and Configuration Management,Interface Specification and Compatibility Bugs,"Higher version torch causes API changes、such as missing attributes or behavior change,Old interface deprecated in new version causes error,After transformers update pytest fails、function deprecated,Scheduler removed causing error、device_type parameter incompatible,Missing torch1.11 causes failure,view() incompatible with old torch,Old API invalid or behavior changed in new PyTorch,test_bias_gelu incompatible with torch 1.12 or diffusers forward interface change triggers crash,pydantic incompatible with config class、PyTorch not installed causes failure,autotuner config change breaks compatibility,Old API example unusable / inference_engine init fails,THCGeneral.h missing / TORCH_CUDA_ARCH_LIST compile conflict / hipify parameter wrong,If mpu/config parameter not provided、Config interface incompatible、scheduler setting abnormal,Not inheriting PyTorch Dataset deemed invalid,AutoProcessor inconsistent with PlaceholderRange,HF template load order wrong,V0 does not support custom config,snake_case and kebabcase naming not supported,PythonicTool accepts alpha names only,Tool name does not support snake_case,Tool name does not support snake_case"
Interface Design and Configuration Management,Interface Tool and Configuration Management Failures,"Path error、special characters unescaped、missing env var causes error,Treating python command as script file,Unescaped characters in MPI training cause config fail or abnormal behavior,Address conflict / fork init failure,OpenMPI unsupported / parameter pass fail,API key parameter conflicts with env var,VLLM_NCCL_SO_PATH env var ineffective for spawn worker,tokenizer file path wrong"
Interface Design and Configuration Management,Configuration Parameter Handling Anomalies,"User config invalid、command line parameter invalid,Setting not applied、default behavior abnormal,User config error、parameter invalid or interface behavior inconsistent,Parameter change triggers recompile or improper batch setup causes evaluation fail,Quantization setting not read correctly,Offload config chaotic、optimizer state inconsistent、gradient_accumulation ineffective,train_batch_size differs from micro_batch、parameter abnormal,model_meta_device wrong、command line quotes unbalanced,Config format wrong、parameter dtype mixed causes allgather error,max_in_cpu parameter invalid,Missing key field in autotune or manual config causes feature missing,Causes model config check fail or runtime interrupt,max_grad_norm config not working,initialize alters model output、zero_stage setting causes memory surge,ZeRO/Optimizer/Lora/Sequence Parallel module behavior abnormal or config ineffective,Missing option to skip ZeRO states、NVMe config no buffer、parameter state disorder etc,tensor_model_parallel_size set to 16 but actually 4,Mutable default parameter modified in init_inference,autotuning with incompatible ZeRO config causes profiling fail,Performance abnormal / infinite recursion / parameter ineffective,Pydantic config not serializable to JSON,save_checkpoint missing parameter / file ordering wrong,Parameters not passed correctly or initial metric empty causes tuning abnormal,local_rank setting wrong,auto-detect-ckpt-format conflict not detected,CUDA_VISIBLE_DEVICES setting invalid,Optional config field unhandled,JSON batch config conditional check improper,seed parameter default not None requires manual set,stop parameter or tensorparallelsize setting wrong,max_model_len setting fail,tokens_total metric config improper cannot display in metrics,Cannot find config format supported by model,Command line parameter abnormal,tensor_parallel_size config slow start,maxwidth 0 breaks page header"
Interface Design and Configuration Management,Configuration Logging and Documentation Consistency Bugs,"Users cannot use help info properly or parameters ignored,Latency or parameter statistics wrong / discrepancy amplified,Spelling、config field、comment error or misleading info,Comment inaccurate、docstring mismatched with behavior、config description missing,CODEOWNER missing / parameter format wrong / PR not submitted,Key build parameter undocumented / script in tutorial missing or invalid,config dump format chaotic,token drop strategy default value misspelled,DeepSeek chat template missing {% endif %},GLM4V template not specified,LoRA template contains wrong char,chat_template conditional leads to inconsistent output,Field name sharing ignores top_logprobs"
Architecture and Module Integration,Module Integration and Compatibility Errors,"Model attribute is None causes AttributeError,Some sub-modules fail to load weights,CLIPVisionModel、BF16、frozen parameters etc. not loaded or written correctly,Structure mismatch causes conversion failure or key not found,Weights differ or structure error after model load,lm_head load fails in HybridEngine,Quantizer error,Pipeline chaining fails,ControlNet injection failed、DiffusionPipeline lacks LoRA support、cannot handle longer prompt,Import failure or missing module error,Cannot import module、runtime symbol not found、cc1plus missing、function arguments incompatible etc.,AttributeError or function/module not found or name error,tokenizer cannot run concurrently with model.generate、pipeline module fails on variable-length input,Module import fails such as op_builder/cpuinfo,Pickle deserialization fails、children attribute missing、module attribute missing,Power9 architecture missing vendor_id causes key error、Windows/CI unsupported for some operations,Transformers log change、torch._six removed、meta tensor incompatible with kernel injection,pip upgrade failure、permission issue、torch/transformers cause device detection error,Calling NoneType.save raises error、meta tensor error、optimizer creation error,get_model_profile path misconfigured cannot output report,Assertion failed / wrong weights / shared-weight conflict,Run failure / model error,Extension module build failed / undefined symbol error,OrderedDict import fails in Python 3.6 causing ImportError,Save failed / multi-GPU save scrambled / slow load,InferenceEngine init missing parameters or validation failed,Checkpoint path not found / file load failed,Missing ops or attribute reference failure triggers AttributeError,Windows build failed、CI missing libaio-dev causes compile failure,Module load failed or misused with no guidance,Checkpoint loading method differs between models,DS compile failed on Windows or Linux,Build flag such as DS_BUILD invalid,Cannot build on Windows,Some features missing on Windows,Install/run fails reporting missing module symbol or header,MOE cannot be combined with SD,OPT or Megatron load fails at inference、incompatible or inconsistent output,Tensor model parallel group not initialized,AWQ launch failed,FlashAttention does not support Alibi,LoRA cannot be restored,Whisper launch crashes,load_weights exception,openai-agents Runner.run_streamed call failed,Fork issue during tokenizer init,Mistral tokenizer incompatible,torch.compile incompatible with specific model,GPUModelRunner missing lora_manager attribute,context_parallel_size with max_position_embeddings triggers error,int3 model run error,Multi-modal processor refactor failed,per_token_group_quant_fp8 kernel shape failure,Preemptions combined with LoRA raises error,Enabling prefix cache and disabling block prefill crashes MLA model,DeepSeek and vllm MTP implementations inconsistent,Qwen2.5-VL architecture unsupported,Granite 3.0 model load failed,DeepseekR1 weight binding error,fp8 not supported,xgrammar version feature difference,transformers cannot recognize multi_modality type,Model type unrecognized,Resource module missing,flash_attn API removed,Unrecognized model architecture causes deployment error,Gemma2 model structure unrecognized,as_classification_model call failed,Multimodal renamed wrapper class triggers input failure,xgrammar speculative decoding crash,MP Utilities issue,Unrecognized model type causes crash,BNB loader load failed,bnb model TP unsupported error,Qwen2.57BInstruct architecture unrecognized,XLMRoberta model load failed,vllm LoRA response mismatches peft/transformers attributes not loaded correctly,Vision module inference failed users request fix plan,Cannot build or execute specific op such as scaled_upper_triang_masked_softmax build failed,Python instance cannot cast to C++ type or TypeError raised,Checkpoint or state_dict save/load logic error causes wrong restore or gradients,Device init error or context not set leads to inability to delete torch payload or create wrong shard,User hits failure using feature such as SmoothQuant apply fail or parameter conflict blocks API,Model conversion or inference raises error,Feature ineffective no operator invoked,Operator fusion failed、process conflict or parameter missing,Weight load error causes garbled output、unrecognized layer during weight conversion、dimension mismatch merging weights、image pull failure causes model load error,Converted weight file format wrong or function abnormal,Parameter count mismatch after model conversion,T5 model in mindspore.nn.transformer lacks Beam_Search inference code so cannot go online,Model weight load failed or API call exception"
Architecture and Module Integration,Core Framework Mechanism Bugs,"Cannot recognize or execute certain APIs (such as compile、sdpa)、leading to initialization or training failure,Variable-length sequences break pipeline parallelism、AutoTP unsupported、input failure,deepspeed.initialize called repeatedly or causes side effects,create_adam in Zero optimizer raises error or NoneType call,backward attribute missing causes runtime error,Optimizer type differs from actual low-level optimizer causing type check and init conflict,CUDA_VISIBLE_DEVICES and similar variables invalid causing unintended GPU use or failure,torch.upsample API change leads to FLOPs calculation error,fixture scope misconfigured、test fails unstopped、test script logic wrong affecting CI or coverage,flops accumulation error or profile logic confused in FLOPsProfiler,autotune JSON default null conflicts with constants,include specifying multiple GPUs but only partially effective,Test failure / regression check cannot complete,Path error in pytest tests/__init__.py causes tests invalid or skipped,FLOPs or MACs calculation inconsistent or wrong,Auto-tuning feature error / batch size check anomaly / process abort,FLOPS profiler report abnormal,Python tarfile vulnerability (CVE-2007-4559) allows directory traversal,Statistics error / unit conversion error / inconsistency,Randomness remains when eval enabled,Flops profiler output error / spelling issue,torch version update causes gather crash,XLA backend FX graph inconsistent,triton kernel output wrong,fp8_utils causes CPU call failure,triton mla kernel handling failed,torch.compile crash,PTX code generation failed,Static shape compilation failed,tracer detects inconsistency、strategy conflict、gradients not released、data race,Resharding cost calculation inaccurate,Improper handling of inplace operations or checkpoint flow causes backward error,Automatic gradient function incomplete tensor type handling,AutoTuning error,Model compilation crashes due to dynamic shape"
Architecture and Module Integration,Architectural Module Execution Failures,"Weights become 0 during training or restore、parameter dimensions scrambled,MLP layer composition causes shape mismatch,Size mismatch and inference anomaly on models like LLama2、Qwen,Expert count greater than device count causes inference error,InferenceEngine cleanup insufficient、int4/int8 model error,Target value not properly tensorized triggers CUDNN_STATUS_MISMATCH error,Odd parameter count causes CUDA memory alignment error,Leads to GPTJ and similar model test failure or key/value tensor anomaly,Input tensor type wrong、meta tensor unhandled、weight dimensions mismatch,Running T5-11B and similar on 24 GB GPU causes OOM or init failure,Non-contiguous tensor causes error,hidden_size mismatch,Qwen model shape mismatch causes crash,Embedding shape mismatch causes runtime error,Qwen/QVQ-72B-Preview module shape mismatch,MiniCPMV image embedding not converted to list causes dimension error,Outputs mismatch weights after AWQ or GPTQ quantization,Qwen2-Audio input dimension error,Shape mismatch when InternVL2 processes vision embeddings,hidden_size or tensor shape invalid leads to RuntimeError,Layer norm or tensor shape mismatch leads to RuntimeError,SPMD solver output shape cannot match causing computation failure,RuntimeError: tensor on different devices"
Architecture and Module Integration,Module Design Logic Bugs,"PEFT model improperly handles embedding layer causing all_reduce error,First stage layer count insufficient、parameter partition wrong or slow init,Module ID unexpected、trace cache invalid causing training interruption,softmax behavior wrong,INT8 quantization logic update omitted,PipelineEngine lacks flatten attribute、wrong isinstance check causes crash,In-place operation triggers requires_grad error、nested loading fails etc.,MLP output shape set to intermediate weight dimension、attention reshape fails,bias add、fused relu/bias/residual operations error or missing tests,Missing parameter description such as checkpointable_layers、code comments unclear,replace_wo_policy parameter type check ineffective、optimizer init cannot use config dict,reshape-copy operation failed,Function name wrong、parameters not initialized、assertion/exception unhandled,Module init order unreasonable causing failure,README incorrect / table parse fail,SelfAttention ignores mask / qkv_out type handling error,Mixtral did not use head_dim config,Qwen2.5 classification model not implemented,Qwen2.5 lacks VL classification model support,Lora kernel order improper,torch.unique misuse in Punica kernel,Neuron config override failed,xgrammar does not support enum,get_multimodal_embedding mismatched with PlaceholderRange,DeciLMConfig JSON sub-schema error,load_weights in adapters.py did not return,mistral tokenizer cannot support structured outputs,LoRA removed but not handled,get_multimodal_embeddings returns nested structure causing cache conflict,MoE graph mode needs optimization,bias_scale missing,Default value sync failed during build,lm_head/base_layer not trainable,Missing prompt_embeds causes embedding fine-tune failure,allocate_slots called in wrong place,LoRA weight mapping error,QKVParallelLinear launch bias error,Redundant softmax in PoolingType.STEP,CLIP encoder layer count check wrong,Missing example_input_array etc. prevents graph logging or stalls run,Trainer lacks add_argparse_args attribute causing AttributeError,super().init() missing leads to class init failure,autoparallel module getitem mis-handles tensor dims,Parameter init order wrong、meta parameters mishandled、module init fail,Context or parameters not set properly during init or construction causing failure,Embedding table or tensor sharding logic error causes accuracy or performance issue,Node merge or strategy generation logic defect causes kernel generation error or perf issue,QKV implementation deviates from classic transformer causing wrong results,lora_weights value insufficient triggers assert failure,mlp.proj not split properly during SmoothQuant,cross-attention wrong results、parameter not applied、memory not fully released,Output dimension mismatch causes wrong output、LoRA module test fails and needs fix,In transformer module dot_product_attention_forward_wrapper generates attention_mask overriding external mask、causing redundancy"
Architecture and Module Integration,Inference and Generation Anomalies,"HybridEngine output abnormal,Inference speed slower than expected,Performance worse than other frameworks such as FasterTransformer,Poor inference quality for GPT-J、GPT-Neox etc.、Contrastive Search generates inconsistent,Silent bug / performance degradation or inconsistency,Model output abnormal / model load failed / inference crash,GPT-J/T5/NeoX and similar produce garbled or corrupted output、performance drop、long-text multi-batch inference inconsistent,Beam Search output degraded,No inference performance improvement,Example run error / cannot reproduce / model output scrambled,Model output chaotic / optimizer load failed,ViT/BERT and similar training performance worsened,MoE model evaluation failed,IMA anomaly,Performance drops after MLA module change,Output abnormal under Qwen2.5 with VLLM config,Model output abnormal after 2:4 sparsity quantization,Output abnormal after model load、likely quantization or load logic error,Results from converted model differ from original model,Specific feature not as expected such as Medusa not supporting Mixtral、Grouped Diverse Beam Search abnormal output"
Code Quality、Logging、Testing and Security,Testing and Quality Assurance Bugs,"Nightly test failed、CI workflow path misconfigured、tests not triggered,CI crashed、tests skipped or issue not reproducible,Nightly CI tests frequently fail、cannot verify commit validity,CI or Nightly tests fail、compile error、dependency missing or wrong path,Tests skipped on certain platforms、release becomes unstable,Unit tests fail、coverage incomplete、new logic uncovered,CI tests skipped,Nightly CI fails on platform,Scripts for spaces or copyright replacement fail、code check or compliance impacted,CI or deployment failed,Nightly CI tests periodically fail or give false positives,Nightly CI tests fail、mis-sent mail、incompatible with torch or transformers update,Nightly CI fails repeatedly,Spelling issue / dead link / description missing,BERT convergence difference,Old version test residue,test_pipe unstable,LoRA bias test causes CI failure,Jenkins report failure,vllm format check fails in CI but passes locally,Docker compose test assertion error,entrypoint tests fail due to model update,pre-commit check failed,Missing source file dependency in tests,CI file_exists timeout,CI build failed,Test case failed,Build failed due to GH200 or CI config issue,isort or yapf error in pre-commit,mypy type check failed,CI outage,yapf module crashed in CI,pytest flaky issue,CI crashes because new logic not covered,CI missing dependencies,pytestpipeline.yaml error,Build task did not pass,Tests skipped,Conflict PR comment misidentified,MambaMixer split missed integration test,YAPF format check failed,multistep test failed need revert,test_broadcast skipped affecting accuracy,Benchmark fails when best_of>1,Test case broken by new parameters,actions or github-script version update caused failure,develop branch fix not released、production 0.6.2 fails,Missing or partial test cases such as pytest not detecting typos,test_fx or test_tracer failed；preprocess step missing causes test fail；test_continuous_batching too long；CI tests fail due to timeout misconfig,PR not created by standard flow causes code quality issue；CI build flow not optimized taking too long,Missing preparatory action causes test fail,Tests fail when enabling specific features such as sequence parallel mode or enable_flash_attention,Deterministic dataloader not removed causes test anomaly、performance metric variance、mount disk data inaccessible causes test fail,CI test failed or feature verification incomplete,Tests not covering optimisation combo such as Flash Attention 2 plus Pipeline Parallelism,Missing gradient check、shard optimizer state dict unsupported,Folder structure chaotic or tests uncovered causing functional issue,pypi release or CI workflow build failed,Unit tests hang、missing tests、added model types cause failure,Unit tests cannot pass or coverage incomplete,Unit tests failed or skipped,git operation failed,Missing or interfering tests cause failures or omissions,Test script or CI config error causes unit tests fail or skip,Missing pytest import causes tests not run,Unit tests hard-code GPU count causing validation fail,Missing unit tests cause context init error、test params not updated、multi-process tests not covering pretrained load,Unit tests fail need hotfix or test code change,Wrongly merged unfinished code WIP PR need revert or fix missing commit,Unit tests fail due to function return change such as get_current_device or test directory naming,Unit test format invalid、cannot run or test logic error such as pytest compatibility,Release workflow skipped or GitHub Action unavailable needs manual intervention,tests timeout when merging branches,PR missing labels causing management difficulty,User requests test scripts update to support new feature,Test report cannot show unit or integration test duration,Test case execution failed or non-deterministic failure such as dependency timing or parameter error,Tests not passing such as Test failed tests or Kill tests or multi-GPU tests,User mistakenly created blank issue such as empty title or body,Unit test failed,Conversion script lacks test coverage,Test case location or validation logic error causes gate failure,Test case cannot pass,Test case cannot pass,Gate system fails to detect error code allowing problematic PR through"
Code Quality、Logging、Testing and Security,Code and Algorithm Implementation Failures,"Naming conflict、spelling error、redundant or wrong type parameter triggers logic failure,Double quotes cause type conversion failure,.gitignore format invalid or path issue causes build failure,Spelling error、unsupported API parameter、name conflict,autotuner.py and similar modules have spelling errors、key parameter not generated、optimization or run flow affected,ds_params_percision_t misspelled,Violates PEP8 or poor code readability,ds_chat refactor breaks old code、name conflict causes CI failure,Multiple concurrent commits cause merge conflict blocking integration,Extension attribute name or parameter spelling error or call mismatch raises AttributeError or TypeError,Spelling error impacts run / misleading comment or log,quantizer missing / logits invalid / CUDA error,Duplicate restore script、format wrong、state disordered、zero_to_fp32 output mismatch,AdamW lacks half attribute,Checkpoint parameter naming inconsistent,Initialization error in optimizer,Missing max_sequence_len attribute,Learning rate multiplier wrong,UUID removal prevents deserialization,torch.compile UUID cache failed,Function naming hinders pytest discovery,num_gpu_blocks not initialized,Missing import statement,rule applies label wrongly,Macro naming conflict,Variable naming error causes checkpoint load fail,Variable f undefined,Variable full_sharded_model_space misspelled causes NameError,token_dispatcher misspelled causes hidden shape error,is_coordinator removed causes torch.py error,Function name misspelled,Variable name unclear,Context manager syntax error,bert_model misspelled,Context manager syntax issue in transformer_block.py,Field spelling error causes log anomaly,Missing __init__.py,SamplingType.BEAM misspelled,removeLabels spelling issue,Bracket missing,Unused variable not cleaned,Sampling type misspelled,Filename contains space,hipify.py overwrites shebang,compile_sizes.copy replacement merge logic wrong,Filename space should be dash,Yapf crash,deprecated decorator wrongly annotated,support_torch_compile decorator erases original class info,enableautotoolchoice misspelled,Script naming error causes version naming confusion,Accidentally introduced commit_id.py or redundant function,Field merge conflict in CI,Spelling error,Hermes2ProToolParser class variable assigned twice causing logic error,Branch merge not cleaned causes code conflict、old torch command adapt issue、branch deletion forces PR resend,Code spelling error causes functional issue、rand() invalid param TypeError,File format wrong、spelling error、import order improper,Duplicate computation、redundant op or undefined var causes error,Duplicate module registration causes crash,Method or variable naming error causes function failure,Codebase redundancy,_FORMAT undefined in logger.py causes crash,Variable undefined、attribute missing、class name mismatched to functionality,Extra files not ignored cause repo chaos,Format check or path match fails,Variable or namespace error causes functional anomaly or wrong display,Function or method re-defined causing redundancy or conflict,Code spelling, indent or logic omission leads to incomplete function or trace failure,Non-standard code format causes functional anomaly or instability,Duplicate registration causes abnormal system behavior,Constant misspelled causes logic anomaly、unused import、code not conforming style,Code and comment format non-standard causing display anomaly or format check fail,Misleading redundant impl in test code affects maintainability,User reports code needs refactor to improve maintainability,Sync PR causes conflict or module version mismatch,Style or spelling error in diffusion code or send_message_to_lark.py,Pre-merge checklist incomplete leads to style inconsistency,Unused import may cause import error,Refactor legacy code triggers error such as trtGptModelInflightBatching.h not cleaned,Code change then compile error,Expert system callback naming error causes exception,Spelling error in code causes functional anomaly,Sample code format inconsistent,Export script has redundant or invalid code block,Script contains redundant or invalid code causing functional anomaly,Referenced script or folder missing leading to unusable feature,Unassigned variable referenced,__all__ import package spec inconsistent,Redundant module not cleaned causing runtime anomaly,Duplicate API name floods warnings"
Code Quality、Logging、Testing and Security,Error Handling and Boundary Condition Bugs,"Logic failure or uncovered edge case such as Domino/all_reduce,Exception type changed without updating handling logic,get_global_rank returns wrong value or error type inconsistent,Result differs from Huggingface、result fluctuation、output anomaly、assertion error,Container policy fails on tuple handling、missing assert、from_blob cache value wrong,Error capture mechanism not generic causing exception swallowed,Assert statement unclear,Version check error causes state mismatch,Metrics structure mismatch,New metric not registered,Prometheus metrics only support old version,Metrics is None,Structured output error metrics inconsistent,Custom percentiles cause KeyError,Prometheus counter accumulation anomaly,Error handling unclear,Undefined field error message vague,Assertion failed,Evaluation script assertion failed,Assertion failed,ValueError message vague,Improper assert on config causes potential issue,Assertion error、TypeError、function parameter missing,Test case lacks edge coverage、flash attention test failed,setStorage out of bounds；duplicate function causes redundancy；index error causes numerical issue；input dimension mismatch raises RuntimeError,Assertion failure stops program execution,KeyError、package import failure or relative or absolute import conflict"
Code Quality、Logging、Testing and Security,Logging、Debugging、and Monitoring Bugs,"Using handle print causes redundant output during training,A large amount of useless warnings or config print messages appear,UserWarning、confusing warning messages、training results hard to debug,Warning repeated printing、log config abnormal or assert not as expected,Printing misleading warning or triton cache warning message repeated,Logger module format error、missing f-string,Such as loss value not averaged、optimizer naming messy、log lacks information、affects debugging and performance evaluation,Program errors but no traceback or diagnostic info causing user unable to locate issue,Trace cache warning cannot be disabled or configuration causes log confusion or user confusion,Error message not actionable,FLOPs、latency、log repeated output or wrong estimation,HTML parse failure / spelling error / log chaos,Multi process duplicate print / too much debug output,Too much debug info when Zero3 starts / assert missing space / print used instead of logger,Includes too much duplicate output、log level or scope not limited、leading to information redundancy,Meaningless else or pass / extra debug print,Residual debug statements cause noise,http display error in CI log,Log unit error,print used instead of logger,Token usage info not returned,False log in checkpointing.py,Duplicate log printing,Logger did not inherit log level,Variable spelling error leads to log distortion,Misleading log output,Spelling error log confusion,Misleading error log,Log format missing left bracket,Misleading error message,Platform detection noisy logs too many,Redundant parameter warning,Log format missing bracket,Output wrongly sent to stderr making user unable to parse,Incomplete exception stack trace,DeepseekScaling print affects run,Log ignores alias field,Display unrelated exception info causing startup failure,RequestMetrics time order error,Excessive logging causes CI crash,Metrics log missing,In idle state speculative decoding and prefix cache logs not filtered,Log level poorly set causing overly verbose information,mean or median e2e latency statistics wrong,Issue with last_token_time and arrival_time in RequestMetrics,Using pdb crashes CI no ruff output、CI no ruff output、mypy output not captured,Format check info unclear,Continuously printing performance logs when idle,Invalid warning output,Preempted sequences still included in log statistics,Logger prints redundant code paths、console output messy,Too much output in log leads to info loss or performance drop,Logging too frequently wastes system resources,Incomplete logging control or wrong type hints cause debugging difficulty,Lack of effective debug info makes issue location difficult,Unnecessary runtime info prompt,Insufficient debug info causes diagnostic difficulty,No output after adding debug statement such as printf,Log content write error or format abnormal,Profiler did not record key performance metrics,Missing or redundant log info makes debugging difficult,Too much terminal output,Using print causes excessive output affecting development,Log did not print specific config,Printed values confusing,Log output inconsistent with reality,Error message unclear or config print mismatched with actual input making issue hard to locate"
Code Quality、Logging、Testing and Security,Runtime Errors,"Runtime error、API missing、CI failure,Nightly CI test failed、inference or training flow interrupted,Subprocess exits abnormally or exit code non-zero,RTD build failed、CI errors frequent,Error during inference,torch tracer error,Deprecated warning generated or runtime exception,Such as DS_BUILD_OPS=1 failure、missing packaging>=20.0、import log side effects etc.,pytest parallel compile deadlock、CI scripts invalid after system upgrade,Inference outputs wrong logits、NaN、garbage values or poor performance,HF change causes CI failure / tensorboardX conflict,Fails on virtual machine or test host,1-bit Adam causes hang,Flops Profiler error,Pipe prof error,See_memory_usage error,profiling on Colab error,SHARP unusable etc.,finalize_model_grads error,test_serialization script hangs,compile failure triggers crash,test_compilation.py cannot run,Unknown runtime environment error appears,Debug mode raises PyTorch error,RequestMetrics object access failed,CUDA error with no clear hint,CI error causes test_multi_step_worker.py crash,benchmark_guided_serving.py error,CI hangs,benchmark script request error,benchmark_serving error,ray version upgrade breaks CI,TypeError when saving model such as cannot serialize ProcessGroup,Test flow causes cache timeout or model weight load failure,Script errors due to unhandled exception such as exit after device set or missing module,Runtime TypeError such as wrong keyword arg、indent error、missing attr etc.,AttributeError exception,AttributeError prevents training from continuing,TypeError thrown,AttributeError occurred,Main process no error message,Program error message,benchmark.sh or test script exception causes performance test or save load invalid,CLI tool segmentation fault、code formatter malfunction、build release flow error,Test failed or function abnormal,Code optimization leads to runtime error such as replacing CudaEvent causes test fail,Process error then not exit properly,Script syntax or Docker env issue causes process auto exit,Process ends without notice or inference result truncated,Script launch failed"
Code Quality、Logging、Testing and Security,Security and Stability Failures,"hostname -I error、shell=True security risk or argument pass failure,ResourceWarning warning、rank logs not separated、resources not released,Zarr checkpoint storage extremely slow,Zero1 executes serially,Communication and computation overlap not enabled,Project maliciously tampered by developer causing functional anomaly,Release pipelines contain race condition,Missing __main__ guard leads to OSError,Engine not destructed leading to cleanup failure,Using pickle file in Pixtral tests poses security risk,Cannot stop with Ctrl+C,Engine destructor not triggered leading to resources not cleaned"
Documentation and Maintainability,Documentation and Standards Gaps,"Documentation inconsistent with reality、flag misspelling、misleading error messages or parameter naming mistakes,MoE auxiliary loss inconsistent with the paper,README path error,Scripts in tutorials or docs support only single-GPU or partial configs,Users cannot find required info or feature description wrong,API documentation missing,README or doc path error,Runtime guideline for Modal container undefined,Unused scripts should be removed,Parameter docs not synchronized,Tool explanations missing,Speculative Decoding tokens metric not included,FusedMoE detail lacks specifics,Dockerfile missing Qwen VL,OpenVINO docs missing command,wake_up not documented,Dockerfile.openvino fix method complex—suggest simplifying,DeepSeek-V3 support unclear,vllm_config update prevents old class registration—users must update model definition from error,Distributed parameter help text inconsistent with actual logic,Torch version not updated in sync,Debug guide lacks common issues、should add links to user-reported issues,mathshepherd model docs do not state embedding support,Parameter renaming not synchronized,Docs do not mention async parameter,Docs do not clearly explain async parameter,Long-text support not specified leading to misuse,OpenAI API upgrade changes behavior—docs need broader support,num_crops undocumented,Script parameter list too long、uncommented code limits features、language barrier descriptions messy、README guidance unclear,Essential features missing (e.g., fp8 linear support)、code unmaintained (e.g., RoBERTa pretraining)、not adapted to new versions (e.g., transformers upgrade incompatibility),Documentation description wrong or missing (e.g., PR standards absent),Install errors due to unclear docs or dependency conflicts (e.g., apex install failure),Legacy usage not removed so docs outdated、tensor parallel description incomplete,Code implementation inconsistent with documentation,Example code fails with old dependencies or needs upgrade for security,Missing ddp training guidance; train scripts lack required parameters; API docs don’t cover new features,Lack of clear guidance causes incorrect implementation,PR discussion difficult,Global option such as no_cuda_ext not working—docs need redesign or use env var,Missing example links prevent viewing、start command inconsistency causes confusion,Code changes not synced to docs causing inconsistency,Users report docs lack feature diagrams or PR creation guidance,Hard to maintain or refactor,API updated but docs not synced causing errors when followed,zero3 chunk search tool and zero optim module lack function docs making use hard,Integration test guide needs more detail,Docs inaccessible or unreadable,Inaccurate doc info causes user misuse,Model list or API Chinese-English inconsistent,Doc-described parameters or features differ from software behavior leading to user error,Version compatibility/doc explanation error causes confusion or failure,README not updated with model support,Doc description mismatches code (e.g., Baichuan-7B doc error) and missing model config causes interface failure,Doc content or command parameter description unclear leading to misuse,README or CONTRIBUTING inaccurate or missing key info,Parameter, hardware or process description wrong causing user failure,Users fail at run time because config item missing in docs,Docs or model files unreachable,Maintenance difficult or error prone,Task Trainer examples lack standards,CPU server code not updated"
Documentation and Maintainability,Documentation Metadata and Annotation Bugs,"FlexGen blog chart abnormal、logger lacks f-string support,Inference tutorial or pipeline tutorial code init error,AzureML example link dead、comments outdated or typos,zero-3 misspelled requres_grad、logic mistake in top2gating,save_16bit_model fails when grad_accum not divisible,Time stats wrong / inaccurate print,illegal memory access / cannot use INT8 model,torch.add(alpha=) crashes / torch.mul signature differs,Wrong stats / large result error / redundant output,Length calc error / batch count mismatch,Infinite loop / scale not updated,nn.Upsample causes get_model_profile() error,Parameter shape mismatch / shard mismatch,Missing analysis info / limited training debug,Assertion failed / config error,Model result inconsistent or not converging after replacement,Three-digit layer model load failed / checkpoint misaligned,Parameter order disorder / layer weights mismatch,Matrix multiplication dim error or exec failure,Longer training time / slow evaluation,callback crash or cannot trace params,reserve_partitioned_swap_space error,IndexError / non-FP tensor cannot checkpoint,moe doc typo,Output change between 0.8.1 and 0.8.0,Github should be GitHub,collect_env version parse fail,stats.py not updated,Error message lacks root cause,Error path unclear,Model start failed without explicit error,Error info not exposed,Function name conflict,requirements file typo,MoE doc typo,Checkpoint naming error (common vs sharded),Error stack trace missing,state_dict and model_optim_rng.pt naming inconsistent,.yml file format error,Error not clearly pointing missing key,CODEOWNERS ordering issue,License header removed,Doc table header overlap,grafana typo causes data source issue,Front-end progress bar display abnormal,chat template missing prints None,Missing VDR comment,tqdm progress misaligned,Gaudi doc index abnormal,Doc heading level exposes hidden parts,Help text misspelling,Doc location guide wrong,ask AI button overlaps doc version selector,Markdown render fails need HTML,Markdown render fails need HTML fix,README typo,NeuronCausalLM typo,Markdown cannot render,GitHub tag not matching code version,Markdown render failure,PDF not updated,PR template render failure hides Fish merge checklist,Markdown render failure affects description display,PR description field empty,Markdown render failure affects checklist,Tensorizer markdown render failure,Raw HTML used instead of markdown,API docs trigger multiple chat calls,Markdown title missing、HTML escape failure affects display,Markdown render failure loses PR description need HTML workaround,Markdown render failure loses PR checklist or doc content,Missing docstrings cause functional issue,Example code not visible、description unclear、variable name wrong,Typos in docs need fix、README.md typo causes inaccurate statement,Spelling error or author name text wrong,Typos or inaccurate dependency notes,Doc typos or missing code comments cause confusion,Typos or poor writing in docs or code,Doc example code raises AssertionError or can’t run,Docstring layout messy; code typos; test file content wrong,Doc typos; wrong variable names in code; wrong parameter names in commands; extra characters in docs,Typos in docs or code,Badge or link not displaying,Example tests incomplete causing data load error、paper link dead、download link wrong、code comment or doc typo,HTML doc format abnormal causes display issue,Docs mismatch release version、typo or chart size issue causes display issue,Badge/chart link broken or metadata missing causes display issue,License incomplete needs update,README, parameter or function name typo causes call failure,README.md or 1D tensor parallel doc has spelling/format/extra char misleading,README.md or service doc has typos or incomplete translation"
Documentation and Maintainability,Documentation Toolchain and Configuration Sync Failures,"Invalid documentation link、mixed quotation marks cause configuration issues,Link invalid、import paths incompatible with future versions,pre-commit modifies non-target files、README/blog Chinese links invalid or 404,CI badges in README invalid、spelling mistake、paper link broken,Symbolic links、PyInstaller incompatible with JIT compile lead to run failure,e.g. engine.py or related doc format update causes render or analysis abnormal,Link in README invalid,Popen.kill creates zombie process / ssh hang,hostfile parsing wrong / CUDA device order confused,Triton library requirements unmet / manual compile error,Pre-build ineffective / zero_to_fp32 conversion failed,init_inference error / ONNX export failed,Import failed / kernel missing / attention crash,Connection refused / connection reset by peer / cc error,Message incomplete / build interrupted,Load fails after save_checkpoint / weight rebuild error / zero_to_fp32 model invalid,Signal handler in launcher overridden,Cannot load old checkpoint、zero_to_fp32 rebuild failed,zero_to_fp32 ignores register_buffer、params misaligned、asyncio write failed,Old checkpoint load failed,All subprocesses exit code 1、cannot locate error,GPU not detected / include command abnormal,Checkpoint download link 410 Gone,README example path invalid,README example script link invalid,Missing .gitmodules file triggers error,README link wrong redirects away,Doc link points to deleted branch thus invalid,Link still points to redirect page,Documentation link invalid,CI path config error,isort version warning,pyproject.toml migration failed no version file,Low isort version causes warning,Missing tool_chat_template_llama3_json.jinja file,ROCm guide link wrong,Release version not pushed to PyPI,CI missing datasets package,datasets missing,CONTRIBUTING.md link wrong,Install docs use wrong command,RunLLM doc path invalid,TPU requirements URL invalid,CI build fails with old URL dependency,Missing chat template file,Transformers upgrade breaks CI,mergify rule config error,Neuron with old torch causes Markdown render fail,.github copied into docker causes version confusion,Docs not updated leading to config error,FastAPI upgrade breaks OpenAPI,openai.types module missing、triton warning typo、mm_limits init missing,During clean build PYTHONPATH not passed、pyproject.toml not sync jinja2、missing OpenTelemetry、missing msgspec causes doc build fail,Slack link expired breaking collaboration flow,Doc link invalid or config error causes AssertionError,Dependency library CVE needs upgrade (e.g. streamlit、gradio),Doc build not triggered / fork repo PR lacks permission,Link error or content missing,Link points wrong repo or content not updated,README link optimization request skipped standard flow causing format non-standard,Doc content error or invalid link blocks user operation,Link invalid or missing config guidance,Invalid link in README prevents resource access,Dead link or incomplete document,Example link in doc invalid or description inaccurate,Link jump in doc invalid,Doc link invalid/step error/spelling error,AutoClass tutorial link 404,Trainer API tutorial link invalid,Missing required config file,Document or page 404 not accessible"
Documentation and Maintainability,User Interaction and Documentation Errors,"Users cannot understand which gradient storage in ZeRO refers to and misuse,Command-line behavior inconsistent with documentation,Running tutorial requires users to manually modify script or parameters due to missing info,Poor user experience / misleading logs,Incompatible with HF Trainer.train() causes run failure,Users unaware of method purpose and misuse such as ds.initialize,Unhandled exception in docs causes front-end 500 error,Exception prompt misleads user,Error information displayed misleads user,Log hint does not guide installation properly,Doc description inconsistent with chat behavior,Doc example cannot run,API server docs and examples incomplete cause usage issues；missing auto-policy error hint hampers debugging,Wrong usage in docs or examples,Users encounter difficulty and cannot pinpoint issue,User accidentally submits blank issue with no content,Script run fails or documentation inaccurate such as wrong GPU count,Docs or example code differ from actual functionality causing execution failure,Lack of clear warning when using deprecated interface"
