Participant,Type,Experience Level,Framework Context,Prompt Category,Thematic Quote,Linked Taxonomy Node(s),Research Insight
U1,User,Beginner,MindSpore + Ascend,Installation & Onboarding,"I tried to install MindSpeed for Ascend, but it lacked Apex. Adding Apex caused version conflicts. It took hours to fix.","A.1, B.1, C.1","Beginners are easily blocked by missing dependency management and lack of error messages. Failed setup halts training completely."
U2,User,Beginner,PyTorch + NVIDIA GPU,Installation Diagnostics,"Model kept crashing until I realized my CUDA version was wrong—doc never said anything.","A.1, C.1","Installation docs lack platform-specific notes, leading to environment mismatches."
U3,User,Intermediate,TensorFlow,Documentation Usability,"Tutorial was outdated; checkpoint format no longer compatible.","B.4, C.6","Outdated docs and incompatible formats confuse users and hinder reproducibility."
U4,User,Beginner,MindSpore,Debugging & Training Control,"Loss was flat. Turns out `reduction=none` broke backprop silently.","A.2, C.5","Default parameter behavior is opaque; documentation doesn't explain its impact on training logic."
U5,User,Advanced,PyTorch + LoRA,Checkpointing & Fine-tuning,"After fine-tuning with LoRA, loading the checkpoint failed due to optimizer states missing.","C.6, C.11","Distributed training and incremental optimizer configs interact in complex ways, with no recovery strategy."
U6,User,Intermediate,DeepSpeed,Training Behavior Monitoring,"Loss jumped mid-epoch without explanation.","A.3, C.4","No indicators of training stability or mechanisms for surfacing anomalies."
U7,User,Beginner,PyTorch,Parameter Configuration,"I just guess parameter combinations or ask others. Docs don't say how they work together.","A.2","Documentation lacks guidance on parameter interactions and task-level tuning, leading to frequent trial-and-error."
U8,User,Intermediate,PyTorch,Feature Misuse,"`torch.save` didn’t restore full weights under ZeRO3.","A.2, A.3","Architectural behavior isn't exposed through interface design, violating user expectations."
U9,User,Advanced,TensorFlow + TPUs,Numerical Errors,"BF16 training on TPU caused loss=1e+38, never recovered.","C.4","Low-precision execution lacks stability control and fallback mechanisms."
U10,User,Beginner,HuggingFace Transformers,Example Mismatch,"Tutorial model used wrong tokenizer for ONNX export, output was unreadable.","A.2, C.6","Example code is disconnected from real-world scenarios; version/test consistency must be verified."
U11,User,Intermediate,PyTorch,Config Overload,"There are too many knobs. I just want a preset for common LLM tasks.","B.2, B.5","Configuration complexity is too high; intelligent recommendations are needed for onboarding."
D1,Developer,Advanced,DeepSpeed + ZeRO3,Training Stability,"Silent bugs in gradients destroy user trust. Once the model breaks, they give up.","A.3, C.2, C.4","Frameworks must prioritize training stability and debuggability."
D2,Developer,Intermediate,MindSpore,Installation & Versioning,"Users often hit environment mismatches between our package and open source tools.","A.1, B.1","Ecosystem fragmentation leads to environment conflicts; version compatibility must be supported."
D3,Developer,Advanced,TensorFlow + Multi-GPU,Scheduling Bottlenecks,"Workspace calculation for dynamic shape ops blocks kernel launch.","C.2","Operator scheduling overhead is too high, causing GPU underutilization."
D4,Developer,Advanced,PyTorch + BF16,Reproducibility,"AllReduce in BF16 loses determinism. BLEU drops unexpectedly.","C.4","Mixed-precision operations lack reproducibility safeguards."
D5,Developer,Advanced,DeepSpeed,Concurrency Errors,"Thread race conditions silently corrupt gradients in multi-device jobs.","C.3, C.11","Multithreaded execution lacks boundary protection and diagnostics."
D6,Developer,Intermediate,MindSpore,Operator Portability,"Matmul broke going from Ascend 910B to 910A. Same API, but no fallback.","B.2, C.8","Operators lack cross-generation compatibility, leading to failures from subtle hardware differences."
D7,Developer,Advanced,DeepSpeed ZeRO3,Behavior & Interface Gap,"torch.save doesn’t work as expected under ZeRO3. Each GPU stores partial weights.","A.2, A.3, C.6","Framework interfaces don't abstract away architectural behaviors."
D8,Developer,Advanced,PyTorch,Logging & Observability,"No useful logs when a NIC fails or HCCL crashes.","C.9","System-level failures lack observability, making recovery difficult."
